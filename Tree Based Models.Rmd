---
title: "Machine Learning with Tree-Based Models in R"
output:
  html_document: default
  html_notebook: default
editor_options:
  chunk_output_type: inline
---

# Classification Trees
Tree based models are useful for making decisions or numeric predictions. 

Interpretable - flow charts, decision trees

Ease of Use 

Accurate.

Decision Tree Terminology: Root Node, Internal Nodes, Leaf Nodes 

We will use recursive partitioning from the `rpart` package to train decision tree models. To visualize the decision trees, we will use `rpart.plot`. 

```{r}
#load required libraries
library(caret)
library(gbm)
library(ipred)
library(Metrics)
library(ModelMetrics)
library(randomForest)
library(ROCR)
library(rpart)
library(rpart.plot)
library(readxl)
library(tidyverse)
```

The following example uses the *default of credit card clients* dataset from the UCI machine learning repository. [Click here for the dataset.](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)

```{r}
#load the dataset (readxl)
credit <- read_xls("data/credit_default.xls", skip = 1)

#examine the dataset
glimpse(credit)

#change the 'default...' variable name
credit <- credit %>%
  rename(default = `default payment next month`)

#rename the 'default' classes
credit$default <- as.factor(ifelse(credit$default == 1, "yes", "no"))

#sample the dataset
credit_sample <- sample_n(tbl = credit, size = 1000, replace = F)

#create tree model
credit_model <- rpart(formula = default ~ ., data = credit_sample, method = "class")

#visualize the tree
rpart.plot(x = credit_model, yesno = 2, type = 0, extra = 0)
```

**Advantages of decision trees:**  
  
* Easy to interpret, understand, and visualize
* Can handle both numerical and categorical features (inputs)
* Can handle missing data elegantly
* Robust to outliers
* Require little data preparation
* Can model non-linearity in the data
* Can be trained quickly on large datasets
  
**Disadvantges**  
  
*Large trees can be difficult to interpret
* High vairance which causes poor model performance
* Easy to overfit
  
**Start the modeling process by making an 80/20 train/test split.**
```{r}
# Total number of rows in the credit data frame
n <- nrow(credit_sample)

# Number of rows for the training set (80% of the dataset)
n_train <- round(.8 * n) 

# Create a vector of indices which is an 80% random sample
set.seed(123)
train_indices <- sample(1:n, n_train)

# Subset the credit data frame to training indices only
credit_train <- credit_sample[train_indices, ]  
  
# Exclude the training indices to create the test set
credit_test <- credit_sample[-train_indices, ]
```

Now let's create a model using the training data.
```{r}
# Train the model (to predict 'default')
credit_model <- rpart(formula = default ~ ., 
                      data = credit_train, 
                      method = "class")

# Look at the model output                      
print(credit_model)
```

## Evaluating Classification Model & Performance

Accuracy measures how often the classifier predicts the class correctly:

$$(n)\ correct\ predictions \over (n)\ total\ data\ points$$

However, we can do better and more often than not need a more detailed look at at the Actual vs. Predicted Class values. We can use a **confusion matrix** from the `caret` package for this.

*confusion matrix function from the modelmetrics package can also be used. The arguments names are different but the principle is the same for each.*

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,  
                        newdata = credit_test,   
                        type = "class")  

# Calculate the confusion matrix for the test set
caret::confusionMatrix(data = class_prediction,       
                reference = credit_test$default)
```

As you may have noticed, the `confusion matrix` function gave us quite a bit of information.  
  
The *accuracy* tells us how well the model does at predicting the classes overall and the *95% CI* gives the confidence intervals around the accuracy. 
  
The *no information rate* tells us how well the model would perform if there were no predictors to use. In other words, how well does the model predict the classes based on random chance. A clear indication of a bad model is if the no information rate is higher than the accuracy.
  
*Sensitivity* inidicates the true positive rate of the model. In our case, the sensitivity is the ability of the test to correctly identify those who **will not** default on their loan.
  
And the *specificity* indicates the true negative rate, providing us with an idnication of how well the test correctly identifies those who **will** default.


## Splitting Criterion In Trees

A classification tree partitions the data so that groups are as homogenous (pure) as possible. From a mathematical standpoint, it makes sense to measure the impurity of the partitioning - the gini index

In the following examples we will use different splitting criterion and compare the classification error do determine which produces the best model

```{r}
# Train a gini-based model
credit_model1 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = 'gini'))

# Train an information-based model
credit_model2 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = 'information'))

# Generate predictions on the validation set using the gini model
pred1 <- predict(object = credit_model1, 
             newdata = credit_test,
             type = "class")    

# Generate predictions on the validation set using the information model
pred2 <- predict(object = credit_model2, 
             newdata = credit_test,
             type = "class")

# Compare classification error (modelmetrics)
ce(actual = credit_test$default, 
   predicted = pred1)
ce(actual = credit_test$default, 
   predicted = pred2)  
```

---

# Regression Trees

The following examples will use a subset of the *Student Performance Dataset* from the UCI Machine Learning Repository. [Download the dataset here.](https://archive.ics.uci.edu/ml/datasets/Student+Performance) 

The goal of this exercise is to predict a student's final mathematics grade based on the following variables: `sex, age, address, studytime (weekly study time), schoolsup (extra educational support), famsup (family educational support), paid (extra paid classes within the course subject) and absences.`
```{r}
#load the dataset
student <- read.csv("data/student.csv", sep = ";")

#examine the dataset
glimpse(student)

#select variables of interest
student <- student %>%
  select(sex, age, address, studytime, schoolsup, famsup, paid, absences, final_grade = G3)
```

Unlike before, we will split the data into three sets rather than two. This will include the training set, validation set, and test set.

The training set has the same function as before. It will be used to build the model. The validation set is used to tune the hyperparameters of a model or select the best model from a list of candidate models. And the test set will be used to test the generalizability of the model. It will be used only once at the very end of the modeling process.

```{r}
#set seed for reproducibility
set.seed(1)

# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15% 
assignment <- sample(1:3, size = nrow(student), prob = c(.7,.15, .15), replace = TRUE)

# Create a train, validation and tests from the original data frame 
student_train <- student[assignment == 1, ]    # subset the student data frame to training indices only
student_valid <- student[assignment == 2, ]  # subset the student data frame to validation indices only
student_test <- student[assignment == 3, ]   # subset the student data frame to test indices only
```

Now, let's use the training data to fit a model. With regression trees, we use `method = 'anova'` as opposed to `method = "class"` for classification trees.

```{r}
# Train the model
student_model <- rpart(formula = final_grade ~ ., 
                     data = student_train, 
                     method = "anova")

# Look at the model output                      
print(student_model)

# Plot the tree model
rpart.plot(x = student_model, yesno = 2, type = 0, extra = 0)
```

## Performance Metrics for Regression

Next, we will use the test set to generate predicted values using the model. And finally we will use the `metrics` package to calculate the RMSE.

**Mean Absolute Error**

$$MAE = {\sum |\ actual - predicted\ |\over n} $$

**Root Mean Square Error**

$$ RMSE = \sqrt{\sum{(\ actual - predicted\ )^2}\over n} $$
 
```{r}
# Generate predictions on a test set
pred <- predict(object = student_model,   # model object 
                newdata = student_test)  # test dataset

# Compute the RMSE
rmse(actual = student_test$final_grade, 
     predicted = pred)
```

hyperparameters for a decision tree
 
rpart.control  
minsplit: minimum number of data points required to attempt a split  
maxdepth: depth of a decision tree  
cp: complexity parameter  

The best sized tree is the model that contains the least error. We can visualize this via the `complexity parameter plot`.

```{r}
# Plot the "CP Table" (rpart)
plotcp(student_model)

# Print the "CP Table"
print(student_model$cptable)

# Retreive optimal cp value based on cross-validated error
(opt_index <- which.min(student_model$cptable[, "xerror"]))

(cp_opt <- student_model$cptable[opt_index, "CP"])

# Prune the model (to optimized cp value)
student_model_opt <- prune(tree = student_model, 
                         cp = cp_opt)
                          
# Plot the optimized model
rpart.plot(x = student_model_opt, yesno = 2, type = 0, extra = 0)
```

grid search for model selection. This helps to find the right hyperparameters.

model hyper parameters are the knobs that you tweak to get slightly different models.

The goal of a grid search is to find the combination of hyper parameters that produce the best model.

```{r}
# Establish a list of possible values for minsplit and maxdepth
minsplit <- seq(1, 4, 1)
maxdepth <- seq(1, 6, 1)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(minsplit = minsplit, maxdepth = maxdepth)

# Check out the grid
head(hyper_grid)

# Print the number of grid combinations
nrow(hyper_grid)
```

And now we will apply this grid of minsplit and maxdepth values to our training set to create a list of models.

```{r}
# Number of potential models in the grid
num_models <- nrow(hyper_grid)

# Create an empty list to store models
student_models <- list()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:num_models) {

    # Get minsplit, maxdepth values at row i
    minsplit <- hyper_grid$minsplit[i]
    maxdepth <- hyper_grid$maxdepth[i]
    
    # Train a model and store in the list
    student_models[[i]] <- rpart(formula = final_grade ~ ., 
                               data = student_train, 
                               method = "anova",
                               minsplit = minsplit,
                               maxdepth = maxdepth)
}
```

And now we can use the validation set to choose the best model from the list of models we created above. To evaluate which model is best we will use RMSE. For other types of models, you can use different evaluation parameters such as classification error, or AUC.

```{r}
# Number of potential models in the grid
num_models <- length(student_models)

# Create an empty vector to store RMSE values
rmse_values <- c()

# Write a loop over the models to compute validation RMSE
for (i in 1:num_models) {

    # Retreive the i^th model from the list
    model <- student_models[[i]]
    
    # Generate predictions on grade_valid 
    pred <- predict(object = model,
                    newdata = student_valid)
    
    # Compute validation RMSE and add to the 
    rmse_values[i] <- rmse(actual = student_valid$final_grade, 
                           predicted = pred)
}

# Identify the model with smallest validation set RMSE
best_model <- student_models[[which.min(rmse_values)]]

# Print the model paramters of the best model
best_model$control
```

Finally we will use the 'best' model on the test set to compute the RMSE.
```{r}
# Compute test set RMSE on best_model
pred <- predict(object = best_model,
                newdata = student_test)
rmse(actual = student_test$final_grade, 
     predicted = pred)
```

# Bagged Trees

As discussed earlier, one of the downsides to tree based models is their tendency to have high variance. Bagging is a method that can be used to get around this.

Bagging is short for Bootstrap Aggregation. From a training set, we can take many bootstrap samples and generate tree models for each. From these models we can generate predictions and with these predictions, we can average them all together to come up with a final outcome. This method of averaging together models is referred to as the ensemble method and is the basis for reducing variance within tree models and preventing overfitting.

The `ipred` package contains functions to create a bagged model. The number of bagged trees can be specified using the `nbagg parameter`, but here we will use the default (25).

If we want to estimate the model's accuracy using the "out-of-bag" (OOB) samples, we can set the the coob parameter to TRUE. The OOB samples are the training obsevations that were not selected into the bootstrapped sample (used in training).

We will explore bagged trees with the credit dataset
```{r}
# Bagging is a randomized model, so let's set a seed (123) for reproducibility
set.seed(123)

# Train a bagged model (ipred)
bagged_credit_model <- bagging(formula = default ~ ., 
                        data = credit_train,
                        coob = TRUE)

# Print the model
print(bagged_credit_model)
```

## Evaluating the bagged tree performance

ROC - 

AUC - metrics package

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = bagged_credit_model,    
                            newdata = credit_test,  
                            type = "class")  # return classification labels

# Print the predicted classes
print(class_prediction)

# Calculate the confusion matrix for the test set (ModelMetrics version)
caret::confusionMatrix(data = class_prediction,
                       reference = credit_test$default)

```


```{r}
# Generate predictions on the test set
pred <- predict(object = bagged_credit_model,
                newdata = credit_test,
                type = "prob")

# `pred` is a matrix
class(pred)
                
# Look at the pred format
head(pred)
                
# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
(credit_ipred_model_test_auc <- auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
    predicted = pred[,"yes"]))
```

## Cross Validating Models

```{r}
# Specify the training configuration
ctrl <- trainControl(method = "cv",     # Cross-validation
                     number = 5,      # 5 folds
                     classProbs = TRUE,                  # For AUC
                     summaryFunction = twoClassSummary)  # For AUC

# Cross validate the credit model using "treebag" method; 
# Track AUC (Area under the ROC curve)
set.seed(1)  # for reproducibility
credit_caret_model <- train(default ~ .,
                            data = credit_train, 
                            method = "treebag",
                            metric = "ROC",
                            trControl = ctrl)

# Look at the model object
print(credit_caret_model)

# Inspect the contents of the model list 
names(credit_caret_model)

# Print the CV AUC
credit_caret_model$results[,"ROC"]
```

```{r}
# Generate predictions on the test set
pred <- predict(object = credit_caret_model, 
                newdata = credit_test,
                type = "prob")

# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
(credit_caret_model_test_auc <- auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
                    predicted = pred[,"yes"]))

```

Compare test set performance to CV performance
```{r}
# Print ipred::bagging test set AUC estimate
print(credit_ipred_model_test_auc)

# Print caret "treebag" test set AUC estimate
print(credit_caret_model_test_auc)
                
# Compare to caret 5-fold cross-validated AUC
credit_caret_model$results[, "ROC"]
```

# Random Forest

* Similar to bagged trees.
    * Both use ensemble modeling to aggregate many models to find the best one
* In Random Forest, only a subset of features are selected at random at each split in a decision tree. 
* In bagging, all features are used.
* reduces correlation between sampled trees
* better performance

We will use the `randomForest` package to create a model using the random forest algorithm. We can also use the `ranger` package.

Again we will utilize the credit dataset in the following examples.
```{r}
#set seed for reproducibility
set.seed(9141)

# Train a random forest
rf_credit_model <- randomForest(formula = default ~ ., data = credit_train)

#examine the random forest
rf_credit_model
```

## Random Forest Model Output
Just as with bagged models, each tree in a Random Forest model uses a bootstrapped sample of the original dataset. The data which is not used in each tree is known as **Out Of Bag** and can be thought of as a validation set. The **OOB** in the Random Forest model summary gives the Out Of Bag error rate. The i'th row in the error rate variable gives the error rate for th samples up to that point and the final row is the overall error of the model.

```{r}
# Grab OOB error matrix & take a look
err <- rf_credit_model$err.rate
head(err)

# Look at final OOB error rate (last row in err matrix)
oob_err <- err[  500, "OOB"]
print(oob_err)

# Plot the model trained in the previous exercise
plot(rf_credit_model)

# Add a legend since it doesn't have one by default
legend(x = "right", 
       legend = colnames(err),
       fill = 1:ncol(err))
```

And now, it's time to evaluate the model performance based on a test set. We will compare this test set error rate (based on the confusion matrix) to the OOB error rate.

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = rf_credit_model,   # model object 
                            newdata = credit_test,  # test dataset
                            type = "class") # return classification labels
                            
# Calculate the confusion matrix for the test set
cm <- caret::confusionMatrix(data = class_prediction,       # predicted classes
                      reference = credit_test$default)  # actual classes
print(cm)

# Compare test set accuracy to OOB accuracy
paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)
```

One of the disadvantages of the Random Forest model is that you will need to utilize a test set to calculate model statistics such as AUC. Why would we want to calculate AUC when we have a built in validation set (OOB)? With AUC, we can compare the Random Forest model to other models.

```{r}
# Generate predictions on the test set
pred <- predict(object = rf_credit_model,
            newdata = credit_test,
            type = "prob")

# `pred` is a matrix
class(pred)
                
# Look at the pred format
head(pred)
                
# Compute the AUC (`actual` must be a binary 1/0 numeric vector)
auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
    predicted = pred[,"yes"])
```

## Random Forest Hyper Parameters
  
* ntree: number of trees
* mtry: number of variables randomly sampled as candidates at each split
* sampsize: number of samples to train on
* nodesize: minimum size (number of samples) of the terminal nodes
* maxnodes: maximum number of terminal nodes  
  
`tuneRF` is `Random Forest's` built in tuning parameter for the `mtry` hyper parameter which allows you to specify how many variables are used at each split. The `tunerf` parameter will iterate over a variety of mtry values stepwise by 2.

```{r}
# Execute the tuning process
set.seed(1)              
res <- tuneRF(x = subset(credit_train, select = -default),
              y = credit_train$default,
              ntreeTry = 500)
               
# Look at results
print(res)

# Find the mtry value that minimizes OOB Error
mtry_opt <- res[,"mtry"][which.min(res[,"OOBError"])]
print(mtry_opt)
```

If you just want to return the best RF model (rather than results) you can set `doBest = TRUE` in `tuneRF()` to return the best RF model instead of a set performance matrix.

```{r}
tuneRF(x = subset(credit_train, select = -default),
              y = credit_train$default,
              ntreeTry = 500,
              doBest = TRUE)
```

Now, let's define some of the other hyper parameters we discussed earlier to tune a random forest model.

```{r}
# Establish a list of possible values for mtry, nodesize and sampsize
mtry <- seq(4, ncol(credit_train) * 0.8, 2)
nodesize <- seq(3, 8, 2)
sampsize <- nrow(credit_train) * c(0.7, 0.8)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)

# Create an empty vector to store OOB error values
oob_err <- c()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {

    # Train a Random Forest model
    model <- randomForest(formula = default ~ ., 
                          data = credit_train,
                          mtry = hyper_grid$mtry[i],
                          nodesize = hyper_grid$nodesize[i],
                          sampsize = hyper_grid$sampsize[i])
                          
    # Store OOB error for the model                      
    oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}

# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
print(hyper_grid[opt_i,])
```

# Boosting

**Adaboost**
Train a decision tree where each observation is assigned an equal weight.
Next, create a tree where the weights of the observations are changed based on:
harder to classify: increase weight, easier to classify: lower the weight

The second tree is grown on the weighted data.

Our new model then becomes Tree 1 + Tree 2 and we can calculate the classification error from this new 2-tree ensemble method.

We then grow a 3rd tree to predict the revised residuals.

This process continues for a specified number of iterations.

Gradient Boosting Machine (GBM)

**GBM = Gradient Descent + Boosting**

Fit an additive model (ensemble) in a forward, stage-wise manner.
In each stage, introduce a "weak learner" (e.g. decision tree) to compensate the shortcomings of existing weak learners.
In Adaboost, "shortcomings" are identified by high-weight data points.
In Gradient Boosting, the "shortcomings" are identified by gradients.

Adavntages:
Often performs better than any other algorithm
Directly optimizes cost function

Disadvantages:
Overfits (need to find a proper stopping point)
Sensitive to extreme values and noise

`gbm` function from the `gbm` package
```{r}
# Convert "yes" to 1, "no" to 0
credit_train$default <- ifelse(credit_train$default == "yes", 1, 0)

# Train a 10000-tree GBM model
set.seed(1)
gbm_credit_model <- gbm(formula = default ~ ., 
                    distribution = "bernoulli", 
                    data = credit_train,
                    n.trees = 10000)
                    
# Print the model object                    
print(gbm_credit_model)

# summary() prints variable importance
summary(gbm_credit_model)
```

## Understanding GBM Model Output

The gbm package uses a `predict()` function to generate predictions from a model, similar to many other machine learning packages in R. When you see a function like `predict()` that works on many different types of input (a GBM model, a RF model, a GLM model, etc), that indicates that `predict()` is an "alias" for a GBM-specific version of that function. The GBM specific version of that function is `predict.gbm()`, but for convenience sake, we can just use `predict()` (either works).

One thing that's particular to the `predict.gbm()` however, is that you need to specify the `number of trees` used in the prediction. There is no default, so you have to specify this manually. For now, we can use the same number of trees that we specified when training the model, which is 10,000 (though this may not be the optimal number to use).

Another argument that you can specify is `type`, which is only relevant to `Bernoulli` and `Poisson` distributed outcomes. When using Bernoulli loss, the returned value is on the log odds scale by default and for Poisson, it's on the log scale. If instead you specify `type = "response"`, then gbm converts the predicted values back to the same scale as the outcome. This will convert the predicted values into probabilities for Bernoulli and expected counts for Poisson.

```{r}
# Since we converted the training response col, let's also convert the test response col
credit_test$default <- ifelse(credit_test$default == "yes", 1, 0)

# Generate predictions on the test set
preds1 <- predict(object = gbm_credit_model, 
                  newdata = credit_test,
                  n.trees = 10000)

# Generate predictions on the test set (scale to response)
preds2 <- predict(object = gbm_credit_model, 
                  newdata = credit_test,
                  n.trees = 10000,
                  type = "response")

# Compare the range of the two sets of predictions
range(preds1)
range(preds2)
```

Compute test set AUC of the GBM model for the two sets of predictions. We will notice that they are the same value. That's because AUC is a rank-based metric, so changing the actual values does not change the value of the AUC.

However, if we were to use a scale-aware metric like RMSE to evaluate performance, we would want to make sure we converted the predictions back to the original scale of the response.

```{r}
# Generate the test set AUCs using the two sets of preditions & compare
auc(actual = credit_test$default, predicted = preds1)  #default
auc(actual = credit_test$default, predicted = preds2)  #rescaled
```

# GBM Hyperparameters

* n.trees: number of trees  
* bag.fraction: proportion of observations to be sampled in each tree  
* n.minobsinnode: minimum number of observations in the trees terminal nodes  
* interaction.depth: maximum nodes per tree  
* shrinkage: learning rate  

**Early Stopping**

Use the gbm.perf() function to estimate the optimal number of boosting iterations (aka n.trees) for a GBM model object using both OOB and CV error. When you set out to train a large number of trees in a GBM (such as 10,000) and you use a validation method to determine an earlier (smaller) number of trees, then that's called "early stopping". The term "early stopping" is not unique to GBMs, but can describe auto-tuning the number of iterations in an iterative learning algorithm.

```{r}
# Optimal ntree estimate based on OOB
ntree_opt_oob <- gbm.perf(object = gbm_credit_model, 
                          method = "OOB", 
                          oobag.curve = TRUE)

# Train a CV GBM model
set.seed(1)
gbm_credit_model_cv <- gbm(formula = default ~ ., 
                       distribution = "bernoulli", 
                       data = credit_train,
                       n.trees = 10000,
                       cv.folds = 2)

# Optimal ntree estimate based on CV
ntree_opt_cv <- gbm.perf(object = gbm_credit_model_cv, 
                         method = "cv")
 
# Compare the estimates                         
print(paste0("Optimal n.trees (OOB Estimate): ", ntree_opt_oob))                         
print(paste0("Optimal n.trees (CV Estimate): ", ntree_opt_cv))
```

Now let's compare the AUC for the models we created above using a test set. First, we'll gather predictions on the test set by utilizing the original gbm credit model we created. The first prediction set will be based on the out of bag optimized number trees while the second prediction set will utilize the cross validated number of trees optimization parameter. 
```{r}
# Generate predictions on the test set using ntree_opt_oob number of trees
preds1 <- predict(object = gbm_credit_model, 
                  newdata = credit_test,
                  n.trees = ntree_opt_oob)
                  
# Generate predictions on the test set using ntree_opt_cv number of trees
preds2 <- predict(object = gbm_credit_model, 
                  newdata = credit_test,
                  n.trees = ntree_opt_cv)   

# Generate the test set AUCs using the two sets of preditions & compare
auc1 <- auc(actual = credit_test$default, predicted = preds1)  #OOB
auc2 <- auc(actual = credit_test$default, predicted = preds2)  #CV 

# Compare AUC 
print(paste0("Test set AUC (OOB): ", auc1))                         
print(paste0("Test set AUC (CV): ", auc2))
```

# Putting it all together

In this tutorial, we've learned about make predictions on classification data using tree based models including: decisionin, bagged, random forest, and gradient boosting. Let's compare the AUC for each of the models to determine which is the best. As a reminder, each model will be trained using the training data, and the predictions will be made using the test set for each model.  

```{r}
#create the decision tree model
credit_model <- rpart(formula = default ~ .,
                      data = credit_train)

#create decision tree predicted values
dt_preds <- predict(object = credit_model, 
                    newdata = credit_test)

#create the bagged tree model
bagged_credit_model <- bagging(formula = default ~ .,
                           data = credit_train)

#make predictions using bagged model on test data
bag_preds <- predict(object = bagged_credit_model,
                     newdata = credit_test)

#create random forest model
rf_credit_model <- randomForest(formula = default ~ ., 
                                data = credit_train)

#make predictions on test data using random forest model
rf_preds <- predict(object = rf_credit_model,
                    newdata = credit_test)

#examine the gbm model
gbm_credit_model$call

#Use the cv optimized predictions computed in th gbm section
gbm_preds <- preds2

# Generate the test set AUCs using the two sets of predictions & compare
actual <- credit_test$default
dt_auc <- auc(actual = actual, predicted = dt_preds)
bag_auc <- auc(actual = actual, predicted = bag_preds)
rf_auc <- auc(actual = actual, predicted = rf_preds)
gbm_auc <- auc(actual = actual, predicted = gbm_preds)

# Print results
sprintf("Decision Tree Test AUC: %.3f", dt_auc)
sprintf("Bagged Trees Test AUC: %.3f", bag_auc)
sprintf("Random Forest Test AUC: %.3f", rf_auc)
sprintf("GBM Test AUC: %.3f", gbm_auc)

```

And so it appears that the Gradient Boosting Model performs just slightly better than the Random Forest model. Perhaps if we had used a Random Forest in which we had tuned the hyper parameters, it would have performed better overall.


We conclude this course by plotting the ROC curves for all the models (one from each chapter) on the same graph. The ROCR package provides the prediction() and performance() functions which generate the data required for plotting the ROC curve, given a set of predictions and actual (true) values.

The more "up and to the left" the ROC curve of a model is, the better the model. The AUC performance metric is literally the "Area Under the ROC Curve", so the greater the area under this curve, the higher the AUC, and the better-performing the model is.

```{r}
# List of predictions
preds_list <- list(dt_preds, bag_preds, rf_preds, gbm_preds)

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(credit_test$default), m)

# Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves")
legend(x = "bottomright", 
       legend = c("Decision Tree", "Bagged Trees", "Random Forest", "GBM"),
       fill = 1:m)
```

