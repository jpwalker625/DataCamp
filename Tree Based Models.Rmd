---
title: "Machine Learning with Tree-Based Models in R"
output:
  html_document: default
  html_notebook: default
editor_options:
  chunk_output_type: inline
---

Tree based models are useful for making decisions or numeric predictions. 

Interpretable - flow charts, decision trees

Ease of Use 

Accurate.

Decision Tree Terminology: Root Node, Internal Nodes, Leaf Nodes 


We will use recursive partitioning from the `rpart` package to train decision tree models.

```{r}
#install.packages("rpart")
#install.packages("rpart.plot")

#load necessary libraries
library(rpart)
library(rpart.plot)
library(tidyverse)
library(gdata)

#load the dataset
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls"
credit <- read.xls(xls = url, 
                   #perl = "C:/Perl/bin/perl.exe", 
                   header= TRUE, 
                   skip = 1)

glimpse(credit)

credit_model <- rpart(formula = default.payment.next.month ~ ., data = credit, method = "class")

rpart.plot(x = credit_model, yesno = 2, type = 0, extra = 0)
```

Advantages of decision trees:

Easy to interpret, understand, and visualize
Can handle both numerical and categorical features (inputs)
Can handle missing data elegantly
Robust to outliers
Require little data preparation
Can model non-linearity in the data
Can be trained quickly on large datasets

Disadvantges
Large trees can be difficult to interpret
High vairance which causes poor model performance
Easy to overfit


Start the modeling process by making an 80/20 train/test split.


## Evaluating Classification Model & Performance

Accuracy measures how often the classifier predicts the class correctly.

(n of correct predictions) / (n total data points)

Confusion Matrix gives a more detailed look at the Actual vs. Predicted Class values.

## Splitting Criterion In Trees

A classification tree partitions the data so that groups are as homogenous (pure) as possible. From a mathematical standpoint, it makes sense to measure the impurity of the partitioning - the gini index

##Regression Trees

We use `method = 'anova'` as an argument to model regression trees.

Performance metrics for regression:

### Mean Absolute Error
 $$MAE = {\sum | actual - predicted |\over n} $$

### Root Mean Square Error
$$ RMSE = \sqrt{\sum{(actual - predicted)^2}\over n} $$
 
 Next, we will use the test set to generate predicted values using the model. And finally we will use the `metrics` package to calculate the rmse.
 
 
 hyperparameters for a decision tree
 
 rpart.control
minsplit: minimum number of data points required to attempt a split
cp: complexity parameter
maxdepth: depth of a decision tree

grid search for model selection. This helps to find the right hyperparameters.

model hyper parameters are the knobs that you tweak to get slightly different models.

The goal of a grid search is to find the combination of hyper parameters that produce the best model.

We can evaluate models by running them on a validation set and choosing the proper performance metric such as classification error, AUC, or RMSE 