---
title: "Machine Learning with Tree-Based Models in R"
output:
  html_document: default
  html_notebook: default
editor_options:
  chunk_output_type: inline
---

# Classification Trees
Tree based models are useful for making decisions or numeric predictions. 

Interpretable - flow charts, decision trees

Ease of Use 

Accurate.

Decision Tree Terminology: Root Node, Internal Nodes, Leaf Nodes 

We will use recursive partitioning from the `rpart` package to train decision tree models. To visualize the decision trees, we will use `rpart.plot`. 

```{r}
#load required libraries
library(caret)
library(ipred)
library(ModelMetrics)
library(rpart)
library(rpart.plot)
library(readxl)
library(tidyverse)
```

The following example uses the *default of credit card clients* dataset from the UCI machine learning repository. [Click here for the dataset.](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)

```{r}
#load the dataset (readxl)
credit <- read_xls("data/credit_default.xls", skip = 1)

#examine the dataset
glimpse(credit)

#change the 'default...' variable name
credit <- credit %>%
  rename(default = `default payment next month`)

#rename the 'default' classes
credit$default <- as.factor(ifelse(credit$default == 1, "yes", "no"))

#sample the dataset
credit_sample <- sample_n(tbl = credit, size = 1000, replace = F)

#create tree model
credit_model <- rpart(formula = default ~ ., data = credit_sample, method = "class")

#visualize the tree
rpart.plot(x = credit_model, yesno = 2, type = 0, extra = 0)
```

**Advantages of decision trees:**  
  
* Easy to interpret, understand, and visualize
* Can handle both numerical and categorical features (inputs)
* Can handle missing data elegantly
* Robust to outliers
* Require little data preparation
* Can model non-linearity in the data
* Can be trained quickly on large datasets
  
**Disadvantges**  
  
*Large trees can be difficult to interpret
* High vairance which causes poor model performance
* Easy to overfit
  
**Start the modeling process by making an 80/20 train/test split.**
```{r}
# Total number of rows in the credit data frame
n <- nrow(credit_sample)

# Number of rows for the training set (80% of the dataset)
n_train <- round(.8 * n) 

# Create a vector of indices which is an 80% random sample
set.seed(123)
train_indices <- sample(1:n, n_train)

# Subset the credit data frame to training indices only
credit_train <- credit_sample[train_indices, ]  
  
# Exclude the training indices to create the test set
credit_test <- credit_sample[-train_indices, ]
```

Now let's create a model using the training data.
```{r}
# Train the model (to predict 'default')
credit_model <- rpart(formula = default ~ ., 
                      data = credit_train, 
                      method = "class")

# Look at the model output                      
print(credit_model)
```

## Evaluating Classification Model & Performance

Accuracy measures how often the classifier predicts the class correctly:

$$(n)\ correct\ predictions \over (n)\ total\ data\ points$$

However, we can do better and more often than not need a more detailed look at at the Actual vs. Predicted Class values. We can use a **confusion matrix** from the `caret` package for this.

*confusion matrix function from the modelmetrics package can also be used. The arguments names are different but the principle is the same for each.*

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,  
                        newdata = credit_test,   
                        type = "class")  

# Calculate the confusion matrix for the test set
caret::confusionMatrix(data = class_prediction,       
                reference = credit_test$default)
```

As you may have noticed, the `confusion matrix` function gave us quite a bit of information.  
  
The *accuracy* tells us how well the model does at predicting the classes overall and the *95% CI* gives the confidence intervals around the accuracy. 
  
The *no information rate* tells us how well the model would perform if there were no predictors to use. In other words, how well does the model predict the classes based on random chance. A clear indication of a bad model is if the no information rate is higher than the accuracy.
  
*Sensitivity* inidicates the true positive rate of the model. In our case, the sensitivity is the ability of the test to correctly identify those who **will not** default on their loan.
  
And the *specificity* indicates the true negative rate, providing us with an idnication of how well the test correctly identifies those who **will** default.


## Splitting Criterion In Trees

A classification tree partitions the data so that groups are as homogenous (pure) as possible. From a mathematical standpoint, it makes sense to measure the impurity of the partitioning - the gini index

In the following examples we will use different splitting criterion and compare the classification error do determine which produces the best model

```{r}
# Train a gini-based model
credit_model1 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = 'gini'))

# Train an information-based model
credit_model2 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = 'information'))

# Generate predictions on the validation set using the gini model
pred1 <- predict(object = credit_model1, 
             newdata = credit_test,
             type = "class")    

# Generate predictions on the validation set using the information model
pred2 <- predict(object = credit_model2, 
             newdata = credit_test,
             type = "class")

# Compare classification error (modelmetrics)
ce(actual = credit_test$default, 
   predicted = pred1)
ce(actual = credit_test$default, 
   predicted = pred2)  
```

---

# Regression Trees

The following examples will use a subset of the *Student Performance Dataset* from the UCI Machine Learning Repository. [Download the dataset here.](https://archive.ics.uci.edu/ml/datasets/Student+Performance) 

The goal of this exercise is to predict a student's final mathematics grade based on the following variables: `sex, age, address, studytime (weekly study time), schoolsup (extra educational support), famsup (family educational support), paid (extra paid classes within the course subject) and absences.`
```{r}
#load the dataset
student <- read.csv("data/student.csv", sep = ";")

#examine the dataset
glimpse(student)

#select variables of interest
student <- student %>%
  select(sex, age, address, studytime, schoolsup, famsup, paid, absences, final_grade = G3)
```

Unlike before, we will split the data into three sets rather than two. This will include the training set, validation set, and test set.

The training set has the same function as before. It will be used to build the model. The validation set is used to tune the hyperparameters of a model or select the best model from a list of candidate models. And the test set will be used to test the generalizability of the model. It will be used only once at the very end of the modeling process.

```{r}
#set seed for reproducibility
set.seed(1)

# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15% 
assignment <- sample(1:3, size = nrow(student), prob = c(.7,.15, .15), replace = TRUE)

# Create a train, validation and tests from the original data frame 
student_train <- student[assignment == 1, ]    # subset the student data frame to training indices only
student_valid <- student[assignment == 2, ]  # subset the student data frame to validation indices only
student_test <- student[assignment == 3, ]   # subset the student data frame to test indices only
```

Now, let's use the training data to fit a model. With regression trees, we use `method = 'anova'` as opposed to `method = "class"` for classification trees.

```{r}
# Train the model
student_model <- rpart(formula = final_grade ~ ., 
                     data = student_train, 
                     method = "anova")

# Look at the model output                      
print(student_model)

# Plot the tree model
rpart.plot(x = student_model, yesno = 2, type = 0, extra = 0)
```

## Performance Metrics for Regression

Next, we will use the test set to generate predicted values using the model. And finally we will use the `metrics` package to calculate the RMSE.

**Mean Absolute Error**

$$MAE = {\sum |\ actual - predicted\ |\over n} $$

**Root Mean Square Error**

$$ RMSE = \sqrt{\sum{(\ actual - predicted\ )^2}\over n} $$
 
```{r}
# Generate predictions on a test set
pred <- predict(object = student_model,   # model object 
                newdata = student_test)  # test dataset

# Compute the RMSE
rmse(actual = student_test$final_grade, 
     predicted = pred)
```

hyperparameters for a decision tree
 
rpart.control
minsplit: minimum number of data points required to attempt a split
maxdepth: depth of a decision tree
cp: complexity parameter

The best sized tree is the model that contains the least error. We can visualize this via the `complexity parameter plot`.

```{r}
# Plot the "CP Table" (rpart)
plotcp(student_model)

# Print the "CP Table"
print(student_model$cptable)

# Retreive optimal cp value based on cross-validated error
(opt_index <- which.min(student_model$cptable[, "xerror"]))

(cp_opt <- student_model$cptable[opt_index, "CP"])

# Prune the model (to optimized cp value)
student_model_opt <- prune(tree = student_model, 
                         cp = cp_opt)
                          
# Plot the optimized model
rpart.plot(x = student_model_opt, yesno = 2, type = 0, extra = 0)
```

grid search for model selection. This helps to find the right hyperparameters.

model hyper parameters are the knobs that you tweak to get slightly different models.

The goal of a grid search is to find the combination of hyper parameters that produce the best model.

```{r}
# Establish a list of possible values for minsplit and maxdepth
minsplit <- seq(1, 4, 1)
maxdepth <- seq(1, 6, 1)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(minsplit = minsplit, maxdepth = maxdepth)

# Check out the grid
head(hyper_grid)

# Print the number of grid combinations
nrow(hyper_grid)
```

And now we will apply this grid of minsplit and maxdepth values to our training set to create a list of models.

```{r}
# Number of potential models in the grid
num_models <- nrow(hyper_grid)

# Create an empty list to store models
student_models <- list()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:num_models) {

    # Get minsplit, maxdepth values at row i
    minsplit <- hyper_grid$minsplit[i]
    maxdepth <- hyper_grid$maxdepth[i]
    
    # Train a model and store in the list
    student_models[[i]] <- rpart(formula = final_grade ~ ., 
                               data = student_train, 
                               method = "anova",
                               minsplit = minsplit,
                               maxdepth = maxdepth)
}
```

And now we can use the validation set to choose the best model from the list of models we created above. To evaluate which model is best we will use RMSE. For other types of models, you can use different evaluation parameters such as classification error, or AUC.

```{r}
# Number of potential models in the grid
num_models <- length(student_models)

# Create an empty vector to store RMSE values
rmse_values <- c()

# Write a loop over the models to compute validation RMSE
for (i in 1:num_models) {

    # Retreive the i^th model from the list
    model <- student_models[[i]]
    
    # Generate predictions on grade_valid 
    pred <- predict(object = model,
                    newdata = student_valid)
    
    # Compute validation RMSE and add to the 
    rmse_values[i] <- rmse(actual = student_valid$final_grade, 
                           predicted = pred)
}

# Identify the model with smallest validation set RMSE
best_model <- student_models[[which.min(rmse_values)]]

# Print the model paramters of the best model
best_model$control
```

Finally we will use the 'best' model on the test set to compute the RMSE.
```{r}
# Compute test set RMSE on best_model
pred <- predict(object = best_model,
                newdata = student_test)
rmse(actual = student_test$final_grade, 
     predicted = pred)
```

# Bagged Trees

As discussed earlier, one of the downsides to tree based models is their tendency to have high variance. Bagging is a method that can be used to get around this.

Bagging is short for Bootstrap Aggregation. From a training set, we can take many bootstrap samples and generate tree models for each. From these models we can generate predictions and with these predictions, we can average them all together to come up with a final outcome. This method of averaging together models is referred to as the ensemble method and is the basis for reducing variance within tree models and preventing overfitting.

The `ipred` package contains functions to create a bagged model. The number of bagged trees can be specified using the `nbagg parameter`, but here we will use the default (25).

If we want to estimate the model's accuracy using the "out-of-bag" (OOB) samples, we can set the the coob parameter to TRUE. The OOB samples are the training obsevations that were not selected into the bootstrapped sample (used in training).

We will explore bagged trees with the credit dataset
```{r}
# Bagging is a randomized model, so let's set a seed (123) for reproducibility
set.seed(123)

# Train a bagged model (ipred)
bagged_credit_model <- bagging(formula = default ~ ., 
                        data = credit_train,
                        coob = TRUE)

# Print the model
print(bagged_credit_model)
```

## Evaluating the bagged tree performance

ROC - 

AUC - metrics package

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = bagged_credit_model,    
                            newdata = credit_test,  
                            type = "class")  # return classification labels

# Print the predicted classes
print(class_prediction)

# Calculate the confusion matrix for the test set (ModelMetrics version)
caret::confusionMatrix(data = class_prediction,
                       reference = credit_test$default)

```


```{r}
# Generate predictions on the test set
pred <- predict(object = bagged_credit_model,
                newdata = credit_test,
                type = "prob")

# `pred` is a matrix
class(pred)
                
# Look at the pred format
head(pred)
                
# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
(credit_ipred_model_test_auc <- auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
    predicted = pred[,"yes"]))
```

## Cross Validating Models

```{r}
# Specify the training configuration
ctrl <- trainControl(method = "cv",     # Cross-validation
                     number = 5,      # 5 folds
                     classProbs = TRUE,                  # For AUC
                     summaryFunction = twoClassSummary)  # For AUC

# Cross validate the credit model using "treebag" method; 
# Track AUC (Area under the ROC curve)
set.seed(1)  # for reproducibility
credit_caret_model <- train(default ~ .,
                            data = credit_train, 
                            method = "treebag",
                            metric = "ROC",
                            trControl = ctrl)

# Look at the model object
print(credit_caret_model)

# Inspect the contents of the model list 
names(credit_caret_model)

# Print the CV AUC
credit_caret_model$results[,"ROC"]
```

```{r}
# Generate predictions on the test set
pred <- predict(object = credit_caret_model, 
                newdata = credit_test,
                type = "prob")

# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
(credit_caret_model_test_auc <- auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
                    predicted = pred[,"yes"]))

```

Compare test set performance to CV performance
```{r}
# Print ipred::bagging test set AUC estimate
print(credit_ipred_model_test_auc)

# Print caret "treebag" test set AUC estimate
print(credit_caret_model_test_auc)
                
# Compare to caret 5-fold cross-validated AUC
credit_caret_model$results[, "ROC"]
```

