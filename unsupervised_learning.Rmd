---
title: "Unsupervised Learning"
output:
  html_document: default
  html_notebook: default
---
```{r setup}
library(tidyverse)
```

# Part 1)

What is unsupervised learning?

3 types of machine learning:

Unsupervised  
* finiding structure in unlabeled data
* finding homogeneous subgroups within groups of the data
* dimensionality reduction

Supervised - making predictions based on labeled data

Reinforcement - computer learns from feedback operating in a real or synthetic environment

*labeled vs. unlabeled data is differentiated by grouping.*

another way to think about this is clustering



## k-means clustering

Used to find homogeneous sub-groups within a population

`kmeans(x, centers = , nstart = )`

one observation per row, one feature per column

k-means has a random component

run multiple time to improve odds of the best model

```{r intro to kmeans}

ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +
         geom_point()

iris_subset <- iris %>% select(Sepal.Length, Sepal.Width)

#kmeans can not take data frames with non-numeric arguments
km.out <- kmeans(x = iris_subset, centers = 3, nstart = 20)

summary(km.out)

#you can print specific components of the kmeans model using the $
km.out$cluster

print(km.out)

#And finally, we can plot the data:
plot(iris_subset, col = km.out$cluster, main = "k-means with 3 clusters, Iris data")
```

How does k-means work?

Random Cluster Assignment
Cluster Centers Are Calculated - Average (mean) Position of all the points of that subgroup (cluster)
This is iteration 1
The algorithm stops when the mean position calculated for each cluster does not change from one iteration to the next


**Model Selection**
Best outcome is based on the total within cluster sum of squares.
Goal is to find the global minimum (minimize the sum of squares)
Similar to calculating the residuals of a regression model

**How to determine the number of clusters without knowing beforehand**
Trial and Error is not the best apporach, but it can help.
"scree plot"

Look for the elbow - this is where the total SS (sum of squares) drastically decreases based on the number of clusters. This elbow point serves as a good approximation of how many clusters to use.

It is important to use the `set.seed` function for reproducibility since k-means uses randomization to assign clusters.

```{r visualizing random k-means algorithms}

# Set up 2 x 3 plotting grid
par(mfrow = c(2, 3))

# Set seed
set.seed(1)

for(i in 1:6) {
  # Run kmeans() on iris_subset with three clusters and one start
  km.out <- kmeans(iris_subset, centers = 3, nstart = 1)
  
  # Plot clusters
  plot(iris_subset, col = km.out$cluster, 
       main = km.out$tot.withinss, 
       xlab = "", ylab = "")
}

#determine how many clusters is optimal

# Initialize total within sum of squares error: wss
wss <- 0

# For 1 to 15 cluster centers
for (i in 1:15) {
  km.out <- kmeans(iris_subset, centers = i, nstart = 20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
}

# Plot total within sum of squares vs. number of clusters
plot(1:15, wss, type = "b", #b for both: points and lines, 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")

# Set k equal to the number of clusters corresponding to the elbow location
k <- 2
```

The default iterations run by kmeans is **10**

When dealing with real data, sometimes you will run into problems/errors in which the number of iterations to find a stopping point is not enough. kmeans takes on the argument `iteration.max` to circumvent this problem.

**another important thing to know is the within cluster sum of squares by cluster parameter**
the between SS/ total SS = some %
The % is the measurment of total variance in the data that is explained by the clustering. 

In the following example we'll using the pokemon data set available from kaggle

https://www.kaggle.com/abcsds/pokemon

```{r kmeans using pokemon data set}
pokemon <- read.csv("data/Pokemon.csv")
  
colnames(pokemon)
#only select columns of interest
pokemon_subset <- pokemon %>% select(6:11)
colnames(pokemon_subset)

#what happens if we use a # of iterations that is insufficient?
kmeans(pokemon_subset, centers = 3, nstart = 20, iter.max = 3)


# Initialize total within sum of squares error: wss
wss <- 0

# Look over 1 to 15 possible clusters
for (i in 1:15) {
  # Fit the model: km.out
  km.out <- kmeans(pokemon_subset, centers = i, nstart = 20, iter.max = 50)
  # Save the within cluster sum of squares
  wss[i] <- km.out$tot.withinss
}

# Produce a scree plot
plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")

# Select number of clusters
k <- 3

# Build model with k clusters: km.out
km.out <- kmeans(pokemon_subset, centers = 3, nstart = 20, iter.max = 50)

# View the resulting model
km.out

# Plot of Defense vs. Speed by cluster membership
plot(pokemon_subset[, c("Defense", "Speed")],
     col = km.out$cluster,
     main = paste("k-means clustering of Pokemon with", k, "clusters"),
     xlab = "Defense", ylab = "Speed")
```


# Part 2)

Hierarchical Clustering is used when the number of clusters is not known ahead of time. 

Bottom-Up and Top-Down clustering types.

Bottom-Up
Each point is a single cluster. The closest clusters are grouped into one cluster. This goes on and on until only one cluster remains.

```{r hierarchical clustering}

#continue with the iris
hclust.out <- hclust(dist(iris_subset))

summary(hclust.out)

#Dendrogram
plot(hclust.out)

#cuttree allows you to cut a hierarchical model. `h` allows you to cut the tree based on a certain height while k denotes a certain number of clusters to cut the tree by. The return values = the cluster number of each observation in the dataset.

# Cut by height
cutree(hclust.out, h = 1.8)

# Cut by number of clusters
cutree(hclust.out, k = 4)

```

How is the distance between clusters determined?
Four methods:
Complete: pairwise similarity between all observations in cluster 1 and 2, and uses largest of similarities.
Single: smallest of ismilarities
Average: average of similarities
Centroid: finds the centroid of cluster 1 and 2, and uses the similarity between two centroids.

Normalize the data if the the data is on different scales

```{r scaling clusters}

# View column means
colMeans(pokemon_subset)

# View column standard deviations
apply(pokemon_subset, FUN = sd, MARGIN = 2)

# Scale the data
pokemon.scaled <- scale(pokemon_subset)
pokemon.scaled
# Create hierarchical clustering model: hclust.pokemon
hclust.pokemon <- hclust(dist(pokemon.scaled), method = "complete")

#Let's quickly recap what you just did. You first checked to see if the column means and standard deviations vary. Because they do, you scaled the data, converted the scaled data to a similarity matrix and passed it into the hclust() function. 

plot(hclust.pokemon)

```

# Part 3)

Dimnesionality Reduction using Principal Components Analysis (PCA)

Three goals in finding lower dimensional representation of features:

* Finding linear combination of variables to create principal components
* maintain the most variance in the data
* The principal components are uncorrelated (orthogonal to each other)

```{r PCA}
pca_pokemon <- pokemon %>% select(HitPoints = HP, Attack, Defense, Speed)

# Perform scaled PCA: pr.out
pr.out <- prcomp(pca_pokemon, scale = TRUE, center = TRUE)

# Inspect model output
summary(pr.out)

#According to the summary above, at least 3 principal components are required to describe at least 75% of the cumulative varaiance in the data.


```
PCA models in R produce additional diagnostic and output components:

center: the column means used to center to the data, or FALSE if the data weren't centered  
scale: the column standard deviations used to scale the data, or FALSE if the data weren't scaled  
rotation: the directions of the principal component vectors in terms of the original features/variables. This information allows you to define new data in terms of the original principal components  
x: the value of each observation in the original dataset projected to the principal components  
You can access these the same as other model components. For example, use pr.out$rotation to access the rotation component.  

## Viualising & Interpreting PCA results

Biplots: 

```{r visualizing PCA results: biplots}

biplot(pr.out)



```

ScreePlots:  
A scree plot shows the variance explained as the number of principal components increases. Sometimes the cumulative variance explained is plotted as well.

```{r visualizing PCA results: scree plots}
pr.out$sdev

# Variability of each principal component: pr.var
pr.var <- pr.out$sdev ^2
pr.var
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
pve

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")

```

## Practical Issues with PCA

Importance of Scaling Data.

Principal Components Can be misguided based on the scale of each component being analyzed.

Take the mtcars dataset for example:

```{r scaling in PCA}

pr.mtcars <- prcomp(mtcars, scale. = FALSE)
biplot(pr.mtcars)

pr.mtcars.scaled <- prcomp(mtcars, scale. = TRUE)
biplot(pr.mtcars.scaled)
```

Here's a more in depth example with the pokemon data

```{r PCA scaling}
pca_pokemon$Total <- pokemon$Total

# Mean of each variable
colMeans(pca_pokemon)

# Standard deviation of each variable
apply(pca_pokemon, 2, sd)

# PCA model with scaling: pr.with.scaling
pr.with.scaling <- prcomp(pca_pokemon, scale. = TRUE)

# PCA model without scaling: pr.without.scaling
pr.without.scaling <- prcomp(pca_pokemon)

# Create biplots of both for comparison
both <- list(pr.with.scaling, pr.without.scaling)

both %>% map(function(y) biplot(y))
```

What we see is that with scaling, the loading vectors are more evenly distributed. The Total variable is not disproportionate like it is in the non-scaled pca model.

# Part 4) CASE STUDY

Let's reinforce what we have already learned by putting it all together.

```{r case study}

#download the data
url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1903/datasets/WisconsinCancer.csv"

wisc.df <- read.csv(url)

# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[ , 3:32])

# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id

# Create diagnosis vector
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
```

## Exploratory Data Analysis
The first step of any data analysis, unsupervised or supervised, is to familiarize yourself with the data.

The variables you created before, wisc.data and diagnosis, are still available in your workspace. Explore the data to answer the following questions:  

1)  How many observations are in this dataset?
2)  How many variables/features in the data are suffixed with _mean?
3)  How many of the observations have a malignant diagnosis?  

```{r EDA}
#1)
glimpse(wisc.data)

#2) 
grep("_mean",colnames(wisc.data))

#3)
count(wisc.df, diagnosis)
#or
sum(wisc.df$diagnosis == "M")
#or
sum(diagnosis)
```

## Performing PCA

The next step in your analysis is to perform PCA on `wisc.data.`

You saw in the last chapter that it's important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data:

The input variables use different units of measurement.
The input variables have significantly different variances.
```{r More PCA}
# Check column means and standard deviations
round(colMeans(wisc.data), 2 )

round(apply(wisc.data, 2, sd), 2)

# Execute PCA, scaling if appropriate: wisc.pr
wisc.pr <- prcomp(wisc.data, scale = TRUE)

# Look at summary of results
summary(wisc.pr)
```

## Interpreting PCA Results

Create a biplot of the wisc.pr data. 

What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r Interpeting PCA Results}
biplot(wisc.pr)

```
Execute the code to scatter plot each observation by principal components 1 and 2, coloring the points by the diagnosis.
Repeat the same for principal components 1 and 3. What do you notice about these plots?

```{r interpreting PCA Results 2}
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[, c(1, 2)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC2")

# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")
```

## Variance Explained

**What is the minimum number of principal components needed to explain 80% of the variance in the data?**

From the scree plot below, it appears to be about 5 Principal Components.
If we look at the summary table from above, We know the 5 Principal Components explains ~85% of the variation in the data.

*As you look at these plots, ask yourself if there's an elbow in the amount of variance explained that might lead you to pick a natural number of principal components.*

```{r Interpreting Scree Plot}
# Set up 1 x 2 plotting grid
par(mfrow = c(1, 2))

# Calculate variability of each component
pr.var <- wisc.pr$sdev ^2
pr.var
# Variance explained by each principal component: pve
pve <- pr.var/sum(pr.var)
pve
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")
```

## Hierarchial Clustering of the Data

```{r Clustering}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)

# Calculate the (Euclidean) distances: data.dist
data.dist <- dist(data.scaled)

# Createa hierarchical clustering model: wisc.hclust
wisc.hclust <- hclust(data.dist, method = "complete")

plot(wisc.hclust)
```

## Selecting Number of Clusters

```{r cluster selection}
# Cut tree so that it has 4 clusters: wisc.hclust.clusters
wisc.hclust.clusters <- cutree(wisc.hclust, 4)

# Compare cluster membership to actual diagnoses
table(wisc.hclust.clusters, diagnosis)

```

## K-means clustering and Comparing Results

Don't forget to scale the data first!

```{r K-means clustering}
# Create a k-means model on wisc.data: wisc.km
wisc.km <- kmeans(x = scale(wisc.data), centers = 2, nstart = 20)

# Compare k-means to actual diagnoses
table(wisc.km$cluster, diagnosis)

# Compare k-means to hierarchical clustering
table(wisc.km$cluster, wisc.hclust.clusters)
```

## Clustering on PCA Results

Recall from earlier exercises that the PCA model required significantly fewer features to describe 80% and 95% of the variability of the data. In addition to normalizing data and potentially avoiding overfitting, PCA also uncorrelates the variables, sometimes improving the performance of other modeling techniques.

Let's see if PCA improves or degrades the performance of hierarchical clustering.

```{r comparison}

# Create a hierarchical clustering model: wisc.pr.hclust
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "complete")

# Cut model into 4 clusters: wisc.pr.hclust.clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, 4)

# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)

# Compare to k-means and hierarchical
table(wisc.hclust.clusters, diagnosis)
table(wisc.km$cluster, diagnosis)
```