---
title: 'Text Mining: Bag of Words'
output:
  html_notebook: default
  html_document: default
---
```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# What is text mining?

The process of distilling **actionable insights** from text.

The text mining workflow:

1) Define problem & specific goals
2) Identify text to be collected
3) Text orginization
4) Feature extraction
5) Analysis
6) Reach an insight, recommendation, or output

Semantic parsing: word order & sentence structure matters

With bag of words, each individual word/phrase is treated separately.

The `qdap` package is useful for counting words. Specifically, the function `freq_terms(text, top)` provides a fast, efficient way of doing so. Let's take a look below.

```{r}
# Load qdap
library(qdap)

#if the qdap package is having trouble with the rJava package, you must direct the system environment to the Java program file
#Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_131')
#library(rJava)

# Create variable new text
new_text <- "DataCamp is the first online learning platform that focuses on building the best learning experience specifically for Data Science. We have offices in Boston and Belgium and to date, we trained over 250,000 (aspiring) data scientists in over 150 countries. These data science enthusiasts completed more than 9 million exercises. You can take free beginner courses, or subscribe for $25/month to get access to all premium courses."

# Print new_text to the console
print(new_text)

# Find the 10 most frequent terms: term_count
term_count <- freq_terms(new_text, 10)

# Plot term_count
plot(term_count)
```

For the following work, we can use the example data set `Data` contained in the qdap package.


```{r}
#define data
df <- qdap::DATA

# View the structure of df
str(df)

# Print out the number of rows in df
nrow(df)

# Isolate text from df: dialogue_df
dialogue_df <- df$state
```

Use the `tm` package to create a word corpus. A corpus is a collection of documents.

There are two kinds of the corpus data type, the permanent corpus, PCorpus, and the volatile corpus, VCorpus. In essence, the difference between the two has to do with how the collection of documents is stored in your computer. In this course, we will use the volatile corpus, which is held in your computer's RAM rather than saved to disk, just to be more memory efficient.

```{r}
# Load tm
library(tm)

# Make a vector source: dialogue_source

dialogue_source <- VectorSource(dialogue_df)
dialogue_source

```

Now that we've converted our vector to a Source object, we pass it to another tm function, VCorpus(), to create our volatile corpus. Pretty straightforward, right?

The VCorpus object is a nested list, or list of lists. At each index of the VCorpus object, there is a PlainTextDocument object, which is essentially a list that contains the actual text data (content), as well as some corresponding metadata (meta). It can help to visualize a VCorpus object to conceptualize the whole thing.

```{r}
# Make a volatile corpus: dialogue_corpus
dialogue_corpus <- VCorpus(dialogue_source)

#Print out dialogue corpus
dialogue_corpus

#subset data corresponding to the 7th line
dialogue_corpus[[7]]

#subset the content correpsonding to the 7th line
dialogue_corpus[[7]][1]
```

Use example data to make a corpus from both a vector and a dataframe:

```{r}
#use qdap example dataset
example_text <- data.frame(num = c(1,2,3), 
                           Author1 = c("yes", "no", "maybe so"), 
                           Author2 = c("Hello", "How are you?", "Am I doing this right?"), stringsAsFactors = FALSE)

# Print example_text to the console
str(example_text)

# Create a DataframeSource on column 2 nd 3: df_source
df_source <- DataframeSource(example_text[ , 2:3])

# Convert df_source to a corpus: df_corpus
df_corpus <- VCorpus(df_source)

# Examine df_corpus
df_corpus

# Create a VectorSource on column 3: vec_source
vec_source <- VectorSource(example_text[,3])

# Convert vec_source to a corpus: vec_corpus
vec_corpus <- VCorpus(vec_source)

# Examine vec_corpus
vec_corpus
```

# Cleaning & Preprocessing Data

## Useful preprocssing functions

There are a number of useful functions available in the `tm` package useful for cleaning up text.

tolower() - actually a base r function  
removePunctuation()  
removeNumbers()  
stripWhiteSpace()  
removeWords()  


```{r cleaning and preprocessing text}
# Create the object: text
text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."

# All lowercase
tolower(text)

# Remove punctuation
removePunctuation(text)

# Remove numbers
removeNumbers(text)

# Remove whitespace
stripWhitespace(text)
```

The qdap package offers other text cleaning functions. Each is useful in its own way and is particularly powerful when combined with the others.

bracketX(): Remove all text within brackets (e.g. "It's (so) cool" becomes "It's cool")  
replace_number(): Replace numbers with their word equivalents (e.g. "2" becomes "two")  
replace_abbreviation(): Replace abbreviations with their full text equivalents (e.g. "Sr" becomes "Senior")  
replace_contraction(): Convert contractions back to their base words (e.g. "shouldn't" becomes "should not")  
replace_symbol() Replace common symbols with their word equivalents (e.g. "$" becomes "dollar")  


```{r cleaning with qdap}
#example text to clean
text

# Remove text within brackets
bracketX(text)

# Replace numbers with words
replace_number(text)

# Replace abbreviations
replace_abbreviation(text)

# Replace contractions
replace_contraction(text)

# Replace symbols with words
replace_symbol(text)

```

## Stop words
Stop words are frequent but often provide little information. The `tm` package contains 174 stop words in the common list.

When using stopwords, you can add your own stop words by making a combinedf vector as in the example below:

```{r stopWords}
# List standard English stop words
stopwords("en")

# Print text without standard stop words
removeWords(text, stopwords("en"))

# Add "coffee" and "bean" to the list: new_stops
new_stops <- c("coffee", "bean", stopwords("en"))

# Remove stop words from text
removeWords(text, new_stops)
```

## Word Stemming

word stemming is another useful preprocessing step.

stem_word()  
stem_completion()  
complete_words()  

```{r wordstem/completion}
# Create complicate
complicate <- c("complicated", "complication", "complicatedly")

# Perform word stemming: stem_doc
stem_doc <- stemDocument(complicate)
stem_doc
# Create the completion dictionary: comp_dict
comp_dict <- "complicate"

# Perform stem completion: complete_text 
complete_text <- stemCompletion(stem_doc, comp_dict)

# Print complete_text
complete_text
```


Here's another example in which we try to perform word stemming on similar words in the same sentence.

```{r}
# create text
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."

#define completed dictionary
comp_dict <- c("In", "a", "complicate", "haste", "Tom", "rush", "to", "fix", "new", "too")

# Remove punctuation: rm_punc
rm_punc <- removePunctuation(text_data)

# Create character vector: n_char_vec
n_char_vec <- unlist(strsplit(rm_punc, split = ' '))

# Perform word stemming: stem_doc
stem_doc <- stemDocument(n_char_vec) 

# Print stem_doc
stem_doc

# Re-complete stemmed document: complete_doc
complete_doc <- stemCompletion(stem_doc, comp_dict) 

# Print complete_doc
complete_doc
```

The `tm_map` function is a useful way to apply cleaning functions to a corpus.

Notice how the tm package functions do not need content_transformer(), but base R and qdap functions do.

Be careful when creating functions; the order of cleaning steps makes a difference.

```{r tm_map }
presidents <- data.frame(pres_debate_raw2012, stringsAsFactors = FALSE)

#examine presidents
str(presidents)

#make presidents a source document
presidents_source <- DataframeSource(presidents)

#make presidents_source into a corpus
presidents_corpus <- VCorpus(presidents_source)

#exampine the presidents_corpus
presidents_corpus[[43]]

#Build a function to clean the corpus
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, stopwords("en"))
  return(corpus)
}

# Apply your customized function to the presidents_corpus: clean_corp
clean_corp <- clean_corpus(presidents_corpus)

# Print out a cleaned up dialogue
clean_corp[[43]][1]

# Print out the same dialogue in original form
presidents[43, 2]

```

## TDM vs. DTM


Document Term Matrix makes each term a column, and each document a row
```{r DTM}
# Create the dtm from the corpus: pres_dtm
pres_dtm <- DocumentTermMatrix(clean_corp)

#print out pres_dtm data
pres_dtm

#convert DTM to a matrix (pres_m)
pres_m <- as.matrix((pres_dtm))

dim(pres_m)

#review a portion of the matrix
pres_m[45:51, 123:130]
```


Where as, a Term Document Matrix makes each row a term, and each column a document.  
```{r TDM}
# Create the TDM from the corpus: pres_tdm
pres_tdm <- TermDocumentMatrix(clean_corp)

#print out pres_tdm data
pres_tdm

#convert TDM to a matrix (pres_m)
pres_m <- as.matrix((pres_tdm))

dim(pres_m)

#review a portion of the matrix
pres_m[123:130, 45:51]
```

# Text Mining Visuals

```{r Text Mining Visuals}

# Calculate the rowSums: term_frequency
term_frequency <- rowSums(pres_m)

# Sort term_frequency in descending order
term_frequency <- sort(term_frequency, decreasing = TRUE)

# View the top 10 most common words
head(term_frequency, 10)

# Plot a barchart of the 10 most common words
barplot(term_frequency[1:10], col = "tan", las = 2)
```

If you are OK giving up some control over the exact preprocessing steps, then a fast way to get frequent terms is with freq_terms() from qdap.

The function accepts a text variable, which in our case is the tweets$text vector. You can specify the top number of terms to show with the top argument, a vector of stop words to remove with the stopwords argument, and the minimum character length of a word to be included with the at.least argument. qdap has its own list of stop words that differ from those in tm. Our exercise will show you how to use either and compare their results.

Making a basic plot of the results is easy. Just call plot() on the freq_terms() object.

```{r}
# Create frequency
frequency <- freq_terms(presidents$dialogue, top = 10, at.least = 3, stopwords = "Top200Words")

# Make a frequency barchart
plot(frequency)

# Create frequency2
frequency2 <- freq_terms(presidents$dialogue, top = 10, at.least = 3, stopwords = tm::stopwords("english"))

# Make a frequency2 barchart
plot(frequency2)
```

## WordClouds

A word cloud is a visualization of terms. In a word cloud, size is often scaled to frequency and in some cases the colors may indicate another measurement. For now, we're keeping it simple: size is related to individual word frequency and we are just selecting a single color.

As you saw in the video, the `wordcloud()` function works like this:

`wordcloud(words, frequencies, max.words = 500, colors = "blue")`

Text mining analyses often include simple word clouds. In fact, they are probably over used, but can still be useful for quickly understanding a body of text!

```{r wordcloud}
# Load wordcloud package
library(wordcloud)

# Print the first 10 entries in term_frequency
head(term_frequency, 10)

# Create word_freqs
word_freqs <- data.frame(term = names(term_frequency), num = term_frequency)

# Create a wordcloud for the values in word_freqs
wordcloud(word_freqs$term, word_freqs$num, max.words = 100, colors = "red")
```

Our wordcloud is a little boring. Let's fix it up a bit by modifying the color scheme.

```{r wordcloud color mod}
set.seed(123)

#print list of colors with colors() function
colors()

#lets take a sample of 5 colors
my_colors <- sample(colors(), 4)

wordcloud(word_freqs$term, word_freqs$num, max.words = 100, colors = my_colors)

```

We can also use prebuilt color palettes such as those offered by `RColorBrewer`

Sequential: Colors ascend from light to dark in sequence  
Qualitative: Colors are chosen for their pleasing qualities together  
Diverging: Colors have two distinct color spectra with lighter colors in between

To change the colors parameter of the wordcloud() function you can use a select a palette from RColorBrewer such as "Greens". The function `display.brewer.all()` will list all predefined color palettes. More information on ColorBrewer (the framework behind RColorBrewer) is available on its [website](http://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3).

The function brewer.pal() allows you to select colors from a palette. Specify the number of distinct colors needed (e.g. 8) and the predefined palette to select from (e.g. "Greens"). Often in word clouds, very faint colors are washed out so it may make sense to remove the first couple from a brewer.pal() selection, leaving only the darkest.

```{r RColorBrewer}
#load RColorBrewer library
library(RColorBrewer)

# List the available colors
display.brewer.all()

# Create purple_orange
purple_orange <- brewer.pal(n = 8, name = "PuOr")

# Drop 2 faintest colors
purple_orange <- purple_orange[-(1:2)]

# Create a wordcloud with purple_orange palette
wordcloud(word_freqs$term, word_freqs$num, max.words = 100, colors = purple_orange)
```

## Visualizing Multiple Documents

### Common Words
`commonality.cloud()` allows you to find common words across multiple documents.

For each document, collapse the text into a single vector. 

Next, combine each new text vector into a single dataframe.

Finally convert the dataframe to a vector source, and then to a corpus.

```{r multiple documents}
# use qdap sample datasets hamlet and raj (romeo and juliet)
hamlet <- hamlet

raj <- raj

#create all_hamlet
all_hamlet <- paste(hamlet$dialogue, collapse = " ")

#create all_raj
all_raj <- paste(raj$dialogue, collapse = " ")

#create all_dialogue
all_dialogue <- c(all_hamlet, all_raj)

#convert to vector source
all_dialogue <- VectorSource(all_dialogue)

#Create all_corpus
all_corpus <- VCorpus(all_dialogue)

inspect(all_corpus)
```

Next, we will create a function to clean up the corpus using the `qdap` & `tm`functions we learned about earlier. 

Then we will make a term document matrix, convert it to a matrix and finally,

we will then use the `commonality.cloud()` function to view the words that are similar between the two documents. The function takes on the argument of a matrix where it the rows are the words and the columns are the documents. RowSums need not be applied here. 
```{r commonality.cloud}
# Clean the corpus
all_clean <- clean_corpus(all_corpus)
inspect(all_clean)

# Create all_tdm
all_tdm <- TermDocumentMatrix(all_clean)

# Create all_m
all_m <- as.matrix(all_tdm)

head(all_m, 10)
tail(all_m, 10)

# Print a commonality cloud
commonality.cloud(all_m, max.words = 100, colors = "steelblue1")

```

### Dissimilar Words

Visualizing dissimilar words is very similar to what we did above except, to keep track of which words come from which document, we add column names to the term document matrix.

```{r dissimilar words}
set.seed(119)

# Give the columns distinct names
colnames(all_tdm) <- c("Hamlet", "Romeo & Juliet")

# Create all_m
all_m <- as.matrix(all_tdm)

# Create comparison cloud
comparison.cloud(all_m, max.words = 50, colors = sample(colors(), 2))

```

## Polarized Tag Cloud

A commonality.cloud() may be misleading since words could be represented disproportionately in one corpus or the other, even if they are shared. In the commonality cloud, they would show up without telling you which one of the corpora has more term occurrences. To solve this problem, we can create a pyramid.plot() from the plotrix package.

We can use the existing matrix consisting of the frequency of words from each document. We will then subset the document to extract out the words that appear in both documents. From there we can find the absolute difference between words from each document and plot various ways using the `pyramid.plot` function. Let's try it out.

```{r pyramid plot}
#create common_words
common_words <- subset(all_m, all_m[ ,1] > 0 & all_m[, 2] > 0)

#create difference
difference <- abs(common_words[,1] - common_words[, 2])

#combine common_words & difference
common_words <- cbind(common_words, difference)

head(common_words)
#Order the df from most differences to least
common_words <- common_words[order(common_words[, 3], decreasing = TRUE), ]

head(common_words)

#create a top_25df to make it easier to plot the top_25 words with the greatest differences
top_25df <- data.frame(Hamlet = common_words[1:25, 1],
                       `R & J` = common_words[1:25, 2],
                       labels = rownames(common_words[1:25, ]))
top_25df

#load plotrix package for pyramid plot
library(plotrix)

pyramid.plot(lx = top_25df$Hamlet,
             rx = top_25df$R...J, 
             labels = top_25df$labels, 
             gap = 14, space = 0.2,
             top.labels = c("Hamlet", "Words", "R&J"), main = "Common Words with the Greatest Difference in Usage Frequency between Two Shakesperean Plays",
             laxlab = NULL,
             raxlab = NULL,
             unit = NULL)
```

## Word Network

Another way to view word connections is to treat them as a network, similar to a social network. Word networks show term association and cohesion. A word of caution: these visuals can become very dense and hard to interpret visually.

In a network graph, the circles are called nodes and represent individual terms, while the lines connecting the circles are called edges and represent the connections between the terms.

`qdap` provides a shorcut for making word networks. The `word_network_plot()` and `word_associate()` functions both make word networks easy!

```{r Word Network}

# Word association
word_associate(hamlet$dialogue,
               match.string = "queen", 
               stopwords = stopwords(kind = "en"),
               network.plot = TRUE, cloud.colors = c("red", "steelblue3"))

# Add title
title(main = "Words Associations to \"queen\" in the text of Shakespeare's Hamlet")

```

# Word Clustering

A simple way to do word cluster analysis is with a dendrogram on your term-document matrix. Once you have a TDM, you can call dist() to compute the differences between each row of the matrix.

Next, you call hclust() to perform cluster analysis on the dissimilarities of the distance matrix. Lastly, you can visualize the word frequency distances using a dendrogram and plot(). Often in text mining, you can tease out some interesting insights or word clusters based on a dendrogram.

A note on sparsity:  

> 
You can limit the number of words in your TDM using `removeSparseTerms()` from `tm`. Why would you want to adjust the sparsity of the TDM/DTM? TDMs and DTMs are sparse, meaning they contain mostly zeros. Remember that 1000 tweets can become a TDM with over 3000 terms! You won't be able to easily interpret a dendrogram that is so cluttered, especially if you are working on more text. A good TDM has between 25 and 70 terms. The lower the sparse value, the more terms are kept. The closer it is to 1, the fewer are kept. This value is a percentage cutoff of zeros for each term in the TDM. 


```{r}
#view the dimension of the presidents term document matrix
dim(pres_tdm)

#evaluate the function  remove sparse terms at 0.95 and 0.975 sparsity
tdm1 <- removeSparseTerms(pres_tdm, sparse = 0.95)

tdm1

tdm2 <- removeSparseTerms(pres_tdm, sparse = 0.975)

tdm2
```

Now that we've applied sparsity to the term document matrix, we can put everythin we've learned together to create a dendrogram of the clustered words.

First we need to convert the tdm to a matrix, then to a data frame in order to apply the distribution function `dist()` to it. Finally we can create an `hclust()` object and plot it.

```{r}
#remove sparse terms from tdm as above
pres_tdm2 <- removeSparseTerms(pres_tdm, sparse = 0.90)

pres_tdm2
#create matrix
tdm_m <- as.matrix(pres_tdm2)

#create data frame
tdm_df <- data.frame(tdm_m)

#create distance (dist) of terms of tdm_df
pres_dist <- dist(tdm_df)

#create a cluster analysis object
hc <- hclust(pres_dist)

plot(hc)
```

### Dendrogram Aesthetics

Use the `dendextend` package to improve the aesthetics of your dendrogram.

```{r}
library(dendextend)

#using the hclust object from above, create an as.dendrogram object
hcd <- as.dendrogram(hc)

labels(hcd)

#change branch colors
hcd <- branches_attr_by_labels(dend = hcd, labels = c("health", "care"))

plot(hcd)
# or this way
clusters <-cutree(hcd, 3)
hcd <- branches_attr_by_clusters(hcd, clusters, c("green", "blue", "orange"))

plot(hcd)

#add cluster rectangles
rect.dendrogram(hcd, k = 3, border = "steelblue")
```

## Word Association

Another way to think about word relationships is with the `findAssocs()` function in the `tm` package. For any given word, `findAssocs()` calculates its correlation with every other word in a TDM or DTM. Scores range from 0 to 1. A score of 1 means that two words always appear together, while a score of 0 means that they never appear together.

To use `findAssocs()` pass in a TDM or DTM, the search term, and a minimum correlation. The function will return a list of all other terms that meet or exceed the minimum threshold.

```{r}
library(ggplot2)
library(ggthemes)
# Create associations
associations <- findAssocs(pres_tdm, "government", 0.5)

# View the venti associations
associations

# Create associations_df
associations_df <- list_vect2df(associations)[,2:3]

# Plot the associations_df values
ggplot(associations_df, aes(y = associations_df[, 1])) + 
  geom_point(aes(x = associations_df[, 2]), 
             data = associations_df, size = 3) + 
  theme_gdocs() +
  labs(title = "Words associated with 'government' with a 50% frequency")
```

## Tokenizing Words

So far we have only created document matrices using single or unigram words. But we can also combine words, also known as tokenizing, to gain more insight into our text mining analysis.

Take not good for example. not good as a phrase or token has a significantly different meaning than each of the words apart.

The `rweka` package is useful for tokenizing words as we will practice below.

```{r}
#load the rweka package
library(RWeka)

# Make tokenizer function 
tokenizer <- function(x){
  NGramTokenizer(x, Weka_control(min = 2, max = 2))}

#create a vector source from the `dialogue` variable of the raj data frame
pres_source <- VectorSource(presidents$dialogue)

#convert the vector source into a corpus
pres_corp <- VCorpus(pres_source)

#clean the corpus using the clean_corp function we created earlier
pres_corp <- clean_corpus(pres_corp)


# Create unigram_dtm using the raj_corpus we created above
unigram_dtm <- DocumentTermMatrix(pres_corp)

# Create bigram_dtm
bigram_dtm <- DocumentTermMatrix(pres_corp, control = list(tokenize = tokenizer))

# Examine unigram_dtm
unigram_dtm

# Examine bigram_dtm
bigram_dtm
```

Notice that our bigram dtm is now larger than the original unigram_dtm.

Let's see how this affects a wordcloud/visualizations.

```{r}
#convert bigram_dtm into a matrix
bigram_matrix <- as.matrix(bigram_dtm)

dim(bigram_matrix)

#take a look at a portion of the matrix
bigram_matrix[40:55, 1500:1515]

#create a frequency table
freq <- colSums(bigram_matrix)

#sort by the most frequent words first
freq <- sort(freq, decreasing = TRUE)

bi_words <- names(freq)
head(bi_words)
#plot a wordcloud

wordcloud(words = bi_words, freq, max.words = 25, rot.per = .45)
```

# Mapping MetaData to your Corpus

It is possible to add the metadata of the document you are trying to convert to a document matrix.
```{r}
custom_reader <- readTabular(mapping = list(content = "dialogue",
                                            id = "person"))
custom_corpus <- VCorpus(DataframeSource(presidents),
                         readerControl = list(reader = custom_reader))

#view the metadata dialogue
custom_corpus[[1]][1]

#view the metadata id
custom_corpus[[1]][2]
```

