Sonar <- Sonar[rows,]
# Identify row to split on: split
split <- round(nrow(Sonar) * .6)
# Create train (60%)
train <- Sonar[1:split, ]
# Create test (40%)
test <- Sonar[(split + 1): nrow(Sonar), ]
# Chunk 7: logistic regression
# Fit glm model: model
model <- glm(Class ~ ., train, family = "binomial")
model
# Predict on test: p
p <- predict(model, test, type = "response")
p
# Chunk 8: Confusion Matrix
# Calculate class probabilities: p_class
p_class <- ifelse(p > .5, "M", "R")
p_class
# Create confusion matrix
confusionMatrix(p_class, test$Class)
# Chunk 9: probabilities/predictions
# Apply threshold of 0.9: p_class
p_class <- ifelse(p > .9, "M", "R")
# Create confusion matrix
confusionMatrix(p_class, test$Class)
# Chunk 10: probabilities/predictions 2
p_class <- ifelse(p> .1, "M", "R")
confusionMatrix(p_class, test$Class)
# Chunk 11: ROC curve
library(caTools)
# Predict on test: p
p <- predict(model, test, type = "response")
# Make ROC curve
colAUC(p, test$Class, plotROC = TRUE)
# Chunk 12: calculating AOC
# traincontrol is part of the caret package
myControl <- trainControl(
method = "cv",
number = 10,
summaryFunction = twoClassSummary,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE
)
# using the mycontrol object above, we can fit a model that uses AUC rather than accuracy to fit/evaluate the model
# Train glm with custom trainControl: model
model <- train(form = Class ~ ., data =  Sonar, method = "glm", trControl = myControl)
# Print model to console
print(model)
# Chunk 13: random forests
library(ranger)
#download the wine quality datasets
whites <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", header = TRUE, sep = ";")
reds <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv", header = TRUE, sep = ";")
whites$color <- factor("white")
reds$color <- factor("red")
wine <-  rbind(whites, reds)
# Fit random forest: model
wine_model <- train(
quality ~ .,
tuneLength = 1,
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Print model to console
wine_model
# Chunk 14: tunelength
# Fit random forest: model
wine_model2 <- train(
quality ~ .,
tuneLength = 3,
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Print model to console
wine_model2
# Plot model
plot(wine_model2)
# Chunk 15: custom tuning grids
# create custom tuning grid
mygrid <- data.frame(mtry = c(2, 3, 7, 11, 12))
# Fit random forest: model
wine_model3 <- train(
quality ~ .,
tuneGrid = mygrid,
data = wine, method = "ranger",
trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)
)
# Print model to console
wine_model3
# Plot model
plot(wine_model3)
# Chunk 16: glmnet
library(glmnet)
# Create custom trainControl: myControl
myControl <- trainControl(
method = "cv", number = 10,
summaryFunction = twoClassSummary,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE
)
glmnet_model <- train(
Class ~ ., #formula
Sonar, #data
method = 'glmnet', #type of model
trControl = myControl)
# Print model to console
glmnet_model
# Print maximum ROC statistic
max(glmnet_model[["results"]])
# Chunk 17: glmnet tuning
glmnet_model2 <- train(
Class ~ .,
Sonar,
tuneGrid = expand.grid(alpha = 0:1,
lambda = seq(0.0001, 1, length = 20)),
method = "glmnet",
trControl = myControl
)
# Print model to console
glmnet_model2
# Print maximum ROC statistic
max(glmnet_model2[["results"]][["ROC"]])
plot(glmnet_model2)
mtcars[sample(1:nrow(mtcars), 10), "hp"]
mtcars[sample(1:nrow(mtcars), 10), "hp"] <- NA
View(mtcars)
1:nrow(mtcars)
rm(mtcars)
breast_cancer <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", col_names = TRUE, sep = ";")
breast_cancer <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = TRUE, sep = ";")
View(breast_cancer)
breast_cancer <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = TRUE, sep = " ")
breast_cancer <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = TRUE, sep = ",")
View(breast_cancer)
breast_cancer <- breast_cancer[-1]
View(breast_cancer)
names(breast_cancer) <- c("Cl.thickness", "Cell.size", "Cell.shape", "Marg.adhesion", "Epith.c.size", "Bare.nuclei", "Bl.cromatin", "Normal.nucleoli", "Mitoses")
View(breast_cancer)
View(breast_cancer)
breast_cancer <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = TRUE, sep = "," )
breast_cancer <- breast_cancer[-1]
names(breast_cancer) <- c("Cl.thickness", "Cell.size", "Cell.shape", "Marg.adhesion", "Epith.c.size", "Bare.nuclei", "Bl.cromatin", "Normal.nucleoli", "Mitoses")
breast_cancer
View(breast_cancer)
breast_cancer <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = TRUE, sep = "," )
View(breast_cancer)
breast_cancer <- breast_cancer[2:10]
View(breast_cancer)
breast_cancer <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = TRUE, sep = "," )
View(breast_cancer)
breast_cancer <- breast_cancer[2:10]
names(breast_cancer) <- c("Cl.thickness", "Cell.size", "Cell.shape", "Marg.adhesion", "Epith.c.size", "Bare.nuclei", "Bl.cromatin", "Normal.nucleoli", "Mitoses")
breast_cancer$Bare.nuclei["?"]
breast_cancer$Bare.nuclei["?"]
breast_cancer[?, "Bare.nuclei"]
breast_cancer["?"" , "Bare.nuclei"]
```
breast_cancer["?" , "Bare.nuclei"]
View(breast_cancer)
breast_cancer$Bare.nuclei["bare.nuclei" == "?"]
breast_cancer$Bare.nuclei["bare.nuclei" == ?]
breast_cancer$Bare.nuclei[breastcancer$bare.nuclei == "?"]
breast_cancer$Bare.nuclei[breast_cancer$bare.nuclei == "?"]
breast_cancer %>% filter(Bare.nuclei == "?")
breast_cancer %>% select %>% (bare.nuclei) %>% filter(Bare.nuclei == "?")
breast_cancer %>% select %>% (Bare.nuclei) %>% filter(Bare.nuclei == "?")
breast_cancer %>% select %>% (Bare.nuclei)
breast_cancer %>% select %>% (Bare.nuclei)
breast_cancer %>% select(Bare.nuclei)
breast_cancer$Bare.nuclei[breast_cancer$Bare.nuclei == "?"]
breast_cancer$Bare.nuclei[breast_cancer$Bare.nuclei == "?"] <- NA
View(breast_cancer)
breast_cancer$Bare.nuclei <- as.integer(breast_cancer$Bare.nuclei)
View(breast_cancer)
data("BreastCancer")
rm(breast_cancer)
View(BreastCancer)
BreastCancer[-.]
BreastCancer[-]
BreastCancer[-1]
BreastCancer[1:-1]
BreastCancer[2:10]
breast_cancer_x <- BreastCancer[2:10]
breast_cancery_y <- BreastCancer$Class
breastcancer_model <- train(x = breast_cancer_x, #another way to  specify the model formula
y = breast_cancery_y, # y = the response variable
method = "glm",
trControl = myControl,
preprocess = "medianImpute")
breastcancer_model <- train(x = breast_cancer_x, #another way to  specify the model formula
y = breast_cancery_y, # y = the response variable
method = "glm",
trControl = myControl,
preProcess = "medianImpute")
as.integer(breast_cancer_x)
map(.x = breast_cancer_x, as.integer)
BreastCancer <- map(.x = breast_cancer_x, as.integer)
breast_cancer_x <- BreastCancer[2:10] #predictors, a.k.a. formula variables
data("BreastCancer")
BreastCancer <- map(.x = breast_cancer_x, as.integer)
BreastCancer <- as.dataframe(map(.x = breast_cancer_x, as.integer))
BreastCancer <- as.data.frame(map(.x = breast_cancer_x, as.integer))
BreastCancer <-  invoke_map_df(.f = as.integer, .x = BreastCancer)
BreastCancer <-  invoke_map(.f = as.integer, .x = BreastCancer)
data("BreastCancer")
BreastCancer <-  map_df(.x = BreastCancer, .f = as.integer())
BreastCancer <-  map_df(.x = BreastCancer, .f = as.integer)
breast_cancer_x <- BreastCancer[2:10] #predictors, a.k.a. formula variables
breast_cancery_y <- BreastCancer$Class # the response variable
View(BreastCancer)
View(breast_cancer_x)
breast_cancery_y <- BreastCancer$Class # the response variable
data("BreastCancer")
data("BreastCancer")
breast_cancer_x <- BreastCancer[2:10] #predictors, a.k.a. formula variables
breast_cancer_x <-  map_df(.x = breast_cancer_x, .f = as.integer)
breast_cancery_y <- BreastCancer$Class # the response variable
breastcancer_model <- train(x = breast_cancer_x, #another way to  specify the model formula
y = breast_cancery_y, # y = the response variable
method = "glm",
trControl = myControl,
preProcess = "medianImpute")
breastcancer_model
stuff <- mtcars %>% split(.$cyl)
library(tidyverse)
stuff <- mtcars %>% split(.$cyl)
stuff <- mtcars %>% split(cyl)
mtcars %>% split(.$mpg)
stuff <- mtcars %>% split(colnames(.))
stuff <- mtcars %>% split(colnames)
stuff <- mtcars %>% split(colnames(.$))
stuff <- mtcars$
ggplot(data = mtcars, aes(x = hp, y = disp)) + geom_point()
stuff <- mtcars$
ggplot(data = mtcars, aes(x = hp, y = disp)) + geom_point()
stuff <- mtcars$
ggplot(data = mtcars, aes(x = hp, y = disp)) +
geom_point()
stuff <- mtcars$
ggplot(data = mtcars, aes(x = hp, y = disp)) +
geom_point()
stuff <- mtcars$
ggplot(data = mtcars, aes(x = hp, y = disp)) +
geom_point()
ggplot(data = mtcars, aes(x = hp, y = disp)) +
geom_point()
stuff <- mtcars$
ggplot(data = mtcars, aes(x = hp, y = disp)) + geom_point()
ggplot(data = mtcars, aes(x = hp, y = disp)) + geom_point()
for i in(colnames(mtcars)){
mtcars %>%
ggplot(., aes(x = hp, y = i)) + geom_point()
}
for i in(colnames(mtcars){
mtcars %>%
ggplot(., aes(x = hp, y = i)) + geom_point()
}
for (i in colnames(mtcars)){
mtcars %>%
ggplot(., aes(x = hp, y = i)) + geom_point()
}
for (i in colnames(mtcars)){
mtcars %>%
ggplot(., aes(x = hp, y = i)) + geom_point()
}
colls <- colnames(mtcars)
for (i in colls){
mtcars %>%
ggplot(., aes(hp, i)) + geom_point()
}
for (i in colls){
mtcars %>%
p <- ggplot(., aes(hp, i)) + geom_point()
print(p)
}
mtcars %>%
p <- ggplot(, aes(hp, i)) + geom_point()
mtcars %>%
p <- ggplot(aes(hp, i)) + geom_point()
mtcars %>%
p <- ggplot(~.,aes(hp, i)) + geom_point()
mtcars %>%
p <- ~ggplot(., aes(hp, i)) + geom_point()
mtcars %>%
~ggplot(., aes(hp, i)) + geom_point()
mtcars %>%
ggplot(., aes(hp, i)) + geom_point()
for (i in colls){
mtcars %>%
ggplot(., aes(hp, i)) + geom_point()
print(p)
}
print(p)
for (i in colls){
mtcars %>%
ggplot(., aes(hp, i)) + geom_point()
}
p <- mtcars %>%
ggplot(., aes(hp, i)) + geom_point()
p
for (i in 1:length(colls)){
p <- mtcars %>%
ggplot(., aes(hp, i)) + geom_point()
p
}
for (i in colls)){
p <- mtcars %>%
ggplot(., aes(hp, i)) + geom_point()
p
}
y <- mtcars %>% list(.$mpg, .$cyl, .$disp)
y
y <- list(.$mpg, .$cyl, .$disp)
map(y)
map(y, ggplot())
map(y, ggplot
map(y, ggplot)
mtcars %>%
map(y, ~ggplot(.,aes(hp, y))+ geom_point())
y <- (.$mpg, .$cyl, .$disp)
y <- (mtcars$mpg, mtcars$cyl, mtcars$disp)
y <- (mtcars$mpg, mtcars$cyl, mtcars$disp)
y <- list(mtcars$mpg, mtcars$cyl, mtcars$disp)
mtcars %>%
map(, ~ggplot(.,aes(hp, y))+ geom_point())
mtcars %>%
map(y, ~ggplot(.,aes(hp, y))+ geom_point())
mtcars %>%
map(y, ggplot(.,aes(hp, y))+ geom_point())
mtcars %>%
map(y, ggplot(.,aes(hp, y = y))+ geom_point())
mtcars %>%
map(y, ~ggplot(., aes(hp,y))+ geom_point())
library(tidyverse)
library(tidyverse)
ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +
geom_point()
iris_subset <- iris %>% select(Sepal.Length, Sepal.Width)
#kmeans can not take data frames with non-numeric arguments
km.out <- kmeans(x = iris_subset, centers = 3, nstart = 20)
summary(km.out)
#you can print specific components of the kmeans model using the $
km.out$cluster
print(km.out)
#And finally, we can plot the data:
plot(iris_subset, col = km.out$cluster, main = "k-means with 3 clusters, Iris data")
# Set up 2 x 3 plotting grid
par(mfrow = c(2, 3))
# Set seed
set.seed(1)
for(i in 1:6) {
# Run kmeans() on iris_subset with three clusters and one start
km.out <- kmeans(iris_subset, centers = 3, nstart = 1)
# Plot clusters
plot(iris_subset, col = km.out$cluster,
main = km.out$tot.withinss,
xlab = "", ylab = "")
}
#determine how many clusters is optimal
# Initialize total within sum of squares error: wss
wss <- 0
# For 1 to 15 cluster centers
for (i in 1:15) {
km.out <- kmeans(iris_subset, centers = i, nstart = 20)
# Save total within sum of squares to wss variable
wss[i] <- km.out$tot.withinss
}
# Plot total within sum of squares vs. number of clusters
plot(1:15, wss, type = "b", #b for both: points and lines,
xlab = "Number of Clusters",
ylab = "Within groups sum of squares")
# Set k equal to the number of clusters corresponding to the elbow location
k <- 2
pokemon <- read.csv("Pokemon.csv")
colnames(pokemon)
#only select columns of interest
pokemon_subset <- pokemon %>% select(6:11)
colnames(pokemon_subset)
#what happens if we use a # of iterations that is insufficient?
kmeans(pokemon_subset, centers = 3, nstart = 20, iter.max = 3)
# Initialize total within sum of squares error: wss
wss <- 0
# Look over 1 to 15 possible clusters
for (i in 1:15) {
# Fit the model: km.out
km.out <- kmeans(pokemon_subset, centers = i, nstart = 20, iter.max = 50)
# Save the within cluster sum of squares
wss[i] <- km.out$tot.withinss
}
# Produce a scree plot
plot(1:15, wss, type = "b",
xlab = "Number of Clusters",
ylab = "Within groups sum of squares")
# Select number of clusters
k <- 3
# Build model with k clusters: km.out
km.out <- kmeans(pokemon_subset, centers = 3, nstart = 20, iter.max = 50)
# View the resulting model
km.out
# Plot of Defense vs. Speed by cluster membership
plot(pokemon_subset[, c("Defense", "Speed")],
col = km.out$cluster,
main = paste("k-means clustering of Pokemon with", k, "clusters"),
xlab = "Defense", ylab = "Speed")
#continue with the iris
hclust.out <- hclust(dist(iris_subset))
summary(hclust.out)
#Dendrogram
plot(hclust.out)
#cuttree allows you to cut a hierarchical model. `h` allows you to cut the tree based on a certain height while k denotes a certain number of clusters to cut the tree by. The return values = the cluster number of each observation in the dataset.
# Cut by height
cutree(hclust.out, h = 1.8)
# Cut by number of clusters
cutree(hclust.out, k = 4)
# View column means
colMeans(pokemon_subset)
# View column standard deviations
apply(pokemon_subset, FUN = sd, MARGIN = 2)
# Scale the data
pokemon.scaled <- scale(pokemon_subset)
pokemon.scaled
# Create hierarchical clustering model: hclust.pokemon
hclust.pokemon <- hclust(dist(pokemon.scaled), method = "complete")
#Let's quickly recap what you just did. You first checked to see if the column means and standard deviations vary. Because they do, you scaled the data, converted the scaled data to a similarity matrix and passed it into the hclust() function.
plot(hclust.pokemon)
getwd()
setwd("C:/workspace/DataCamp")
pokemon.csv
"pokemon.csv"
pca_pokemon <- pokemon %>% select(HP = HitPoints)
pca_pokemon <- pokemon %>% select(HitPoints = HP)
pca_pokemon <- pokemon %>% select(HitPoints = HP, Attack, Defense, Speed)
pca_pokemon <- pokemon %>% select(HitPoints = HP, Attack, Defense, Speed)
# Perform scaled PCA: pr.out
pr.out <- prcomp(pokemon, scale = TRUE, center = TRUE)
pr.out <- prcomp(pca_pokemon, scale = TRUE, center = TRUE)
summary(pr.out)
pca_pokemon <- pokemon %>% select(HitPoints = HP, Attack, Defense, Speed)
# Perform scaled PCA: pr.out
pr.out <- prcomp(pca_pokemon, scale = TRUE, center = TRUE)
# Inspect model output
summary(pr.out)
biplot(pr.out)
pve
pr.var <- pr.out$sdev^2
pve <- pr.var / sum(pr.var)
pve
pr.var
pr.out
pr.out$sdev
pr.var
pr.out$sdev
# Variability of each principal component: pr.var
pr.var <- pr.out$sdev ^2
pr.var
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
pve
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
pr.out
summary(pr.out)
plot(summary(pr.out))
pr.mtcars <- prcomp(mtcars, scale. = FALSE)
biplot(pr.mtcars)
pr.mtcars.scaled <- prcomp(mtcars, scale = TRUE)
pr.mtcars.scaled
biplot(pr.mtcars.scaled)
pr.mtcars.scaled <- prcomp(mtcars, scale. = TRUE)
biplot(pr.mtcars.scaled)
pca_pokemon$Total <- pokemon$Total
colMeans(pokemon)
colMeans(pca_pokemon)
apply(pca_pokemon, 2, sd)
pr.with.scaling <- prcomp(pca_pokemon, scale. = TRUE)
pr.without.scaling <- prcomp(pca_pokemon)
both <- c(pr.with.scaling, pr.without.scaling)
map(both, biplot)
map(both, biplot())
map(.x = both, .f = biplot)
biplot(pr.with.scaling)
map(both, ~biplot)
map(both, function(y) biplot(y))
map(both, function(y) biplot(y = y))
both %>%
map(function(y) biplot(y = y))
both <- list(pr.with.scaling, pr.without.scaling)
both %>%
map(function(y) biplot(y = y))
both %>%
map(function(y)biplot())
both %>%
map(function(y) biplot)
both %>%
map(function(y) biplot(y))
both %>%
map(biplot(y))
map(both, biplot)
map(biplot, both)
both
both %>% map(function(y) biplot)
both %>% map(function(y) biplot(y))
both %>% map(function(y) biplot(y))
both %>% map(function(y) biplot(y))
summary(pr.mtcars)
save.image("unsupervisedlearning.rdata")
