---
title: "Supervised Learning"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

All of the datasets used in the examples come from the `OIdata` and `openintro` packages.

```{r eval=FALSE}
# install.packages("devtools")
library(devtools)  
  
install_github("OpenIntroOrg/openintro-r-package", subdir = "OIdata")  
  
install_github("OpenIntroOrg/openintro-r-package", subdir = "openintro")
```

For more information on Open Intro, visit the [website](https://www.openintro.org/) or the [github page](https://github.com/OpenIntroOrg/openintro-r-package) for more information.

# What is regression? 

In the statistical sense, regression is predicting an expected value from a set of inputs.

In the causal sense, regression is predicting a numerical outcome which distinguishes it from classification in which you are predicting a discrete outcome such as Yes or No.

We call the expected outcome the "dependent variable". It is dependent on the inputs, also called the predictors, or "independent variables".

The fundamental principle of linear regression

Change in Y is linearly proportional to change in X.
Each X contributes additively to Y.
Y is the sum of all of the weighted inputs.

A linear regression model requires training data and a formula.

In R, you can specify a formula using the tilde `~`. The variable on the left side is always the dpeendent vartiable and isd a function of the right side, the explanatory, or independent value.

```{r}
library(tidyverse)
library(broom)
library(openintro)

#examine the training data
glimpse(textbooks)

#create a formula
fmla <- formula('amazNew ~ uclaNew')

#examine the formula
fmla
```

Now that we have a formula we can use it to create a model on our dataset
```{r}
#create linear model
textbooks_model <- lm(formula = fmla, data = textbooks)

#examine the model
textbooks_model
```

Once you have fit the model, you can examine it various ways. 

base R: summary()
```{r}
summary(textbooks_model)
```

using the broom package, the tidy way: glance()
```{r}
glance(textbooks_model)
```

Using the stats package (preloaded in R), we can use the `predict` function to find the predicted value for each x based on a specified model.
```{r}
textbooks$predictions <- predict(object = textbooks_model, newdata = textbooks)

head(textbooks)
```

Let's visualize how the predicted values stack up against the observed values
```{r}
ggplot(textbooks, aes(x = predictions, y = amazNew)) +
  geom_point() +
  geom_abline(color = "blue")
```

And we can use the `predict` function to make a prediction for a new dataset or observation. 

```{r}
#make dataset of new textbooks to make predictions on
new_textbooks <- data.frame(uclaNew = c(65, 142))
```

```{r}
#make Amazon price predictions on the new textbooks based on UCLA price
new_textbooks$amazNew <- predict(object = textbooks_model, newdata = new_textbooks)

#get slope and intercept of original model
intercept <- textbooks_model$coefficients[1]
slope <- textbooks_model$coefficients[2]

#plot results
ggplot(textbooks, aes(x = uclaNew, y = amazNew)) +
  geom_point() +
  geom_point(data = new_textbooks, aes(x = uclaNew, y = amazNew), size = 2.5, color = "#ba0000") +
  geom_abline(intercept = intercept, slope = slope, color = "blue")
```


**Pros of Linear Regression**

* Easy to fit and apply  
* Concise - (don't need much storage)  
* Less prone to overfitting (training and test data usually behave similarly)  
* Interpretable  

**Cons** 

* Can only express linear and additive relationships  
* Colinearity   
    + input variables are partially correlated (weight and age)  
    + coefficients might change sign  
    + coefficients (or standard errors) look too large  
    + model may be unstable  

## Model Evaluation

**The following section will explain the parameters which are used to evaluate a model's performance.**  
  
We will use the `bac` dataset which analyzes the relationship between the blood alcohol content (bac) and number of beers drank for 16 students.

```{r}
glimpse(bac)

#model the grades as a function of TV
bac_mod <- lm(BAC ~ Beers, bac)

glance(bac_mod)
bac$pred <- predict(bac_mod, bac)

ggplot(bac, aes(x = BAC, y = pred)) + 
  geom_point()

```


### Residuals
  
The **residuals** Tell you how far off the actual value is from the predicted value. They can also tell you whether there are correlations in your data still unaccounted for. You should expect to see no systematic errors in the plot of the residuals meaning that the amount of error is consistent and random for the entire dataset.

The residual for any obervation is calcualted as the difference between the actual and predicted value.

```{r}
bac$residuals <- bac$BAC - bac$pred

ggplot(bac, aes(x = pred, y = residuals)) +
  geom_pointrange(aes(ymin= 0, ymax = residuals)) +
  geom_hline(yintercept = 0, linetype = 3) +
  ggtitle("residuals vs. linear model prediction")
```


### Gain Curve
  
**Gain Curve** viualizes the sort order of the predictions. The y-axis plots the cumulative sum of the response variable as a fraction and the x-axis plots the numbers of observations as a fraction. This is useful in situations where the sort order is more important than the actual values.  
  
Use the `WVPlots` package for the gain curve function.

```{r}
library(WVPlots)

GainCurvePlot(frame = bac, xvar = "pred", truthVar = "BAC", title = "Blood Alcohol Content Model")
```


### RMSE  
  
**Root Mean Squared Error*. How much error is associated with the prediction values compared to the actual values. You can compare the RMSE to the SD of the actual values to get an idea of how well your model is at predicting the values.

The RMSE is calculated as 

$$RMSE = \sqrt{mean(residual^2)}$$


```{r}
#calculate RMSE
sqrt(mean(bac$residuals^2))

#calculate sd of the original data
sd(bac$BAC)

```

### $R^2$

The $R^2$ is a measure of the goodness of fit, or rather, how well does your model explain the data? The $R^2$ value ranges between 0 - 1. A value of 0 means that you'd be better off guessing what the estimated value would be. A value of 1 means the model fits well and that you've accounted for all the variation in the data.

$$R^2 = 1 - {RSS \over SS_{tot}} $$
where 

RSS is the residual sum of squares  (the variance from the model)
  $$RSS = \sum(y - prediction)^2$$
and  
  
TSS is the total sum of squares (the variance from the data)
$$SS_{tot} = \sum(y - \bar y)^2$$

```{r}
#calculate rss
rss <- sum((bac$BAC - bac$pred)^2)

#calculate tss
tss <- sum((bac$BAC - mean(bac$BAC))^2)

#calculate r_squared
r_squared <- 1 - (rss/tss)

#compare r_squared to the r_squared from glance() function and from the correlation coefficient
r_squared

glance(bac_mod)$r.squared

rho <- cor(y = bac$BAC, x = bac$pred)
rho^2
```

## Model Training

In general, a model performs much better on the training data than on data the model has not yet seen.

It is common to split the dataset into a **training/test** set. This is recommended when the data is plentiful.

**Test/Train splitting tests the final model**

The following examples use the `diamonds` dataset, part of the `ggplot2` package.
```{r}
#model the data (price vs. carat)
glimpse(diamonds)

#get the number of rows in bac
nrows <- nrow(diamonds)

#calculate 75% of the nrows 
target <- round(0.75 * nrows)

#generate a vector of nrow uniform random variables
gp <- runif(n = nrows)

#split the data into test/training sets
train <- diamonds[gp <  0.75, ]
test <- diamonds[gp > .75, ]

nrow(train)
nrow(test)

#create model useing training data
dmnds_mod <- lm(price ~ carat, train)

summary(dmnds_mod)

#predict price from carat on the training set
train$pred <- predict(dmnds_mod, train)

#predict price from carat on the test set
test$pred <- predict(dmnds_mod, test)

#Evaluate the RMSE for each
rmse <- function(predcol, ycol) {
  res <- predcol- ycol
  sqrt(mean(res^2))
}

(rmse_train <- rmse(train$pred, train$price))
(rmse_test <- rmse(test$pred, test$price))

#evaluate the $R^2$ for each
rsq <- function(predcol, ycol) {
  tss = sum( (ycol - mean(ycol))^2 )
  rss = sum( (predcol - ycol)^2 )
  1 - rss/tss
}

(rsq_train <- rsq(train$pred, train$price))
(rsq_test <- rsq(test$pred, test$price))

#plot the predicted vs. the outcome for the test data
ggplot(test, aes(x = pred, y = price))+
  geom_point() +
  geom_abline(color = "blue")
```


If the data set does not have enough data to split off a test set, the **cross-validation** method is preferred.

One way of creating a cross-validation plan is to use the `kWayCrossValidation` function from the `vtreat` package.

**Cross-validation tests the modeling process**

The following examples use the `helmet` dataset from the `openintro` package. The helmet dataset describes the relationship between socioeconomic status measured as the percentage of children in a neighborhood receiving reduced-fee lunches at school(lunch) and the percentage of bike riders in the neighborhood wearing helmets (helmet). 

```{r}
#install.packages("vtreat")
library(vtreat)

#examine the helmet dataset
glimpse(helmet)

#count nrows of helmet
helmet_rows <- nrow(helmet)

#create cross-validtion model with 3-folds
splits <- kWayCrossValidation(nRows = helmet_rows, nSplits = 3)

#examine splits
str(splits)

#run the 3-fold cross-validation plan on the data
helmet$cv_pred <- 0

for(i in 1:3){
  split <- splits[[i]]
  model <- lm(lunch ~ helmet, helmet[split$train, ])
  helmet$cv_pred[split$app] <- predict(model, helmet[split$app, ])
}

#predict from a full model
helmet$pred <- predict(lm(lunch ~ helmet, helmet))

#get RMSE of full model and CV set
rmse(helmet$pred, helmet$lunch)

rmse(helmet$cv_pred, helmet$lunch)
```

## Issues to Consider 

**categorical variables** - one-hot-coding

The model will leave out one of the categories as this is the reference-level. The coefficient value tells you the change expected when all other variables are 0 relative to the reference level.

**interactions**

Recall that linear regression assumes an additive relationship between variables. 

plant height ~ sunlight + bacteria

Change in height is the sum of the effects of bacteria and light. Therefore, 
Change in sunlight causes the same change in height, independent of bacteria. And also,
change in bactera causes the same change in height, independent of sunlight.

**An interaction occurs when the simultaneous influence of two variables on the outcome is not additive.*

In a formula, you can specify interacion with a colon (:)
`r y ~ a:b`

You can specify main effects AND interaction with an asterisk (*)
`r y ~ a*b`
is the same as
`r y ~ a + b + a:b`

And finally, you can express the product of two variables using: I(*)
`r y ~ I(a*b)`

## Transformations

log transformations are usueful in monetary values. Often have wide dynamic range and long tails.

