---
title: "Supervised Learning"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

All of the datasets used in the examples come from the `OIdata` and `openintro` packages.

```{r eval=FALSE}
# install.packages("devtools")
library(devtools)  
  
install_github("OpenIntroOrg/openintro-r-package", subdir = "OIdata")  
  
install_github("OpenIntroOrg/openintro-r-package", subdir = "openintro")
```

For more information on Open Intro, visit the [website](https://www.openintro.org/) or the [github page](https://github.com/OpenIntroOrg/openintro-r-package) for more information.

# What is regression? 

In the statistical sense, regression is predicting an expected value from a set of inputs.

In the causal sense, regression is predicting a numerical outcome which distinguishes it from classification in which you are predicting a discrete outcome such as Yes or No.

We call the expected outcome the "dependent variable". It is dependent on the inputs, also called the predictors, or "independent variables".

The fundamental principle of linear regression

Change in Y is linearly proportional to change in X.
Each X contributes additively to Y.
Y is the sum of all of the weighted inputs.

A linear regression model requires training data and a formula.

In R, you can specify a formula using the tilde `~`. The variable on the left side is always the dpeendent vartiable and isd a function of the right side, the explanatory, or independent value.

```{r}
library(tidyverse)
library(broom)
library(openintro)

#examine the training data
glimpse(textbooks)

#create a formula
fmla <- formula('amazNew ~ uclaNew')

#examine the formula
fmla
```

Now that we have a formula we can use it to create a model on our dataset
```{r}
#create linear model
textbooks_model <- lm(formula = fmla, data = textbooks)

#examine the model
textbooks_model
```

Once you have fit the model, you can examine it various ways. 

base R: summary()
```{r}
summary(textbooks_model)
```

using the broom package, the tidy way: glance()
```{r}
glance(textbooks_model)
```

Using the stats package (preloaded in R), we can use the `predict` function to find the predicted value for each x based on a specified model.
```{r}
textbooks$predictions <- predict(object = textbooks_model, newdata = textbooks)

head(textbooks)
```

Let's visualize how the predicted values stack up against the observed values
```{r}
ggplot(textbooks, aes(x = predictions, y = amazNew)) +
  geom_point() +
  geom_abline(color = "blue")
```

And we can use the `predict` function to make a prediction for a new dataset or observation(s). 

```{r}
#make dataset of new textbooks to make predictions on
new_textbooks <- data.frame(uclaNew = c(65, 142))
```

```{r}
#make Amazon price predictions on the new textbooks based on UCLA price
new_textbooks$amazNew <- predict(object = textbooks_model, newdata = new_textbooks)

#get slope and intercept of the model
intercept <- textbooks_model$coefficients[1]
slope <- textbooks_model$coefficients[2]

#plot results
ggplot(textbooks, aes(x = uclaNew, y = amazNew)) +
  geom_point() +
  geom_point(data = new_textbooks, aes(x = uclaNew, y = amazNew), size = 2.5, color = "#ba0000") +
  geom_abline(intercept = intercept, slope = slope, color = "blue")
```


**Pros of Linear Regression**

* Easy to fit and apply  
* Concise - (don't need much storage)  
* Less prone to overfitting (training and test data usually behave similarly)  
* Interpretable  

**Cons** 

* Can only express linear and additive relationships  
* Colinearity   
    + input variables are partially correlated (weight and age)  
    + coefficients might change sign  
    + coefficients (or standard errors) look too large  
    + model may be unstable  

# Model Evaluation

**The following section will explain the parameters which are used to evaluate a model's performance.**  
  
We will use the `bac` dataset which analyzes the relationship between the blood alcohol content (bac) and number of beers drank for 16 students.

```{r}
#examine the dataset
glimpse(bac)

#visualize the relationship between variables of interest
ggplot(bac, aes(x = Beers, y = BAC)) +
  geom_point()

#model the BAC as a function of beers drank
bac_mod <- lm(BAC ~ Beers, bac)

#View the model stats
glance(bac_mod)

#Make predictions on the original data using our model
bac$pred <- predict(bac_mod, bac)

#visualize the predicted value vs. the actual value
ggplot(bac, aes(x = pred, y = BAC)) + 
  geom_point() +
  geom_abline(linetype = 2)
```

### residuals
  
The **residuals** Tell you how far off the actual value is from the predicted value. They can also tell you whether there are correlations in your data still unaccounted for. You should expect to see no systematic errors in the plot of the residuals meaning that the amount of error is consistent and random for the entire dataset.

The residual for any obervation is calcualted as the difference between the actual and predicted value.

```{r}
#visualize the difference between the actual and predicted value
ggplot(bac, aes(x = Beers, y = BAC)) +
  geom_point() +
  geom_abline(slope = bac_mod$coefficients[2],
              intercept = bac_mod$coefficients[1]) +
  geom_pointrange(aes(ymin = pred, ymax = BAC))

#calculate the residuals
bac$residuals <- bac$BAC - bac$pred

#visualize the variance of the residuals
ggplot(bac, aes(x = pred, y = residuals)) +
  geom_pointrange(aes(ymin= 0, ymax = residuals)) +
  geom_hline(yintercept = 0, linetype = 3) +
  ggtitle("residuals vs. linear model prediction")
```


### Gain Curve
  
**Gain Curve** viualizes the sort order of the predictions. The y-axis plots the cumulative sum of the response variable as a fraction and the x-axis plots the numbers of observations as a fraction. This is useful in situations where the sort order is more important than the actual values.  
  
Use the `WVPlots` package for the gain curve function.

```{r}
library(WVPlots)

GainCurvePlot(frame = bac, xvar = "pred", truthVar = "BAC", title = "Blood Alcohol Content Model")
```


### RMSE  
  
**Root Mean Squared Error*. How much error is associated with the prediction values compared to the actual values. You can compare the RMSE to the SD of the actual values to get an idea of how well your model is at predicting the values.

The RMSE is calculated as 

$$RMSE = \sqrt{mean(residual^2)}$$

Below, we'll compare the RMSE to the standard deviation of the original dataset. Which is better?
```{r}
#calculate RMSE
sqrt(mean(bac$residuals^2))

#calculate sd of the original data
sd(bac$BAC)

```

### $R^2$

The $R^2$ is a measure of the goodness of fit, or rather, how well does your model explain the data? The $R^2$ value ranges between 0 - 1. A value of 0 means that you'd be better off guessing what the estimated value would be. A value of 1 means the model fits well and that you've accounted for all the variation in the data.

$$R^2 = 1 - {RSS \over SS_{tot}} $$
where 

RSS is the residual sum of squares  (the variance from the model)
  $$RSS = \sum(y - prediction)^2$$
and  
  
TSS is the total sum of squares (the variance from the data)
$$SS_{tot} = \sum(y - \bar y)^2$$

```{r}
#calculate rss
rss <- sum((bac$BAC - bac$pred)^2)

#calculate tss
tss <- sum((bac$BAC - mean(bac$BAC))^2)

#calculate r_squared
r_squared <- 1 - (rss/tss)

#compare r_squared to the r_squared from glance() function and from the correlation coefficient
r_squared

glance(bac_mod)$r.squared

rho <- cor(y = bac$BAC, x = bac$pred)
rho^2
```

## Model Training

In general, a model performs much better on the training data than on data the model has not yet seen.

It is common to split the dataset into a **training/test** set. This is recommended when the data is plentiful.

**Test/Train splitting tests the final model**

The following examples uses fictitious tips data. We seek to model the monetary tip associated with a single group as a function of the bill total.
```{r}
#examine the dataset
glimpse(tips)

#get the number of rows in diamonds
nrows <- nrow(tips)

#calculate 75% of the nrows 
target <- round(0.75 * nrows)

#generate a vector of nrow uniform random variables
gp <- runif(n = nrows)

#split the data into test/training sets
train <- tips[gp <  0.75, ]
test <- tips[gp > .75, ]

nrow(train)
nrow(test)

#create model useing training data
tips_mod <- lm(tip ~ bill, train)

summary(tips_mod)

#predict tips from bill total on the training set
train$pred <- predict(tips_mod, train)

#predict tips from bill total on the test set
test$pred <- predict(tips_mod, test)

#Evaluate the RMSE for each
rmse <- function(predcol, ycol) {
  res <- predcol- ycol
  sqrt(mean(res^2))
}

(rmse_train <- rmse(train$pred, train$price))
(rmse_test <- rmse(test$pred, test$price))

#evaluate the $R^2$ for each
rsq <- function(predcol, ycol) {
  tss = sum( (ycol - mean(ycol))^2 )
  rss = sum( (predcol - ycol)^2 )
  1 - rss/tss
}

(rsq_train <- rsq(train$pred, train$price))
(rsq_test <- rsq(test$pred, test$price))

#plot the predicted vs. the outcome for the test data
ggplot(test, aes(x = pred, y = tip))+
  geom_point() +
  geom_abline(color = "blue")
```


If the data set does not have enough data to split off a test set, the **cross-validation** method is preferred.

One way of creating a cross-validation plan is to use the `kWayCrossValidation` function from the `vtreat` package.

**Cross-validation tests the modeling process**

The following examples use the `helmet` dataset from the `openintro` package. The helmet dataset describes the relationship between socioeconomic status measured as the percentage of children in a neighborhood receiving reduced-fee lunches at school(lunch) and the percentage of bike riders in the neighborhood wearing helmets (helmet). 

```{r}
#install.packages("vtreat")
library(vtreat)

#examine the helmet dataset
glimpse(helmet)

#count nrows of helmet
helmet_rows <- nrow(helmet)

#create cross-validtion model with 3-folds
splits <- kWayCrossValidation(nRows = helmet_rows, nSplits = 3)

#examine splits
str(splits)

#run the 3-fold cross-validation plan on the data
helmet$cv_pred <- 0

for(i in 1:3){
  split <- splits[[i]]
  model <- lm(lunch ~ helmet, helmet[split$train, ])
  helmet$cv_pred[split$app] <- predict(model, helmet[split$app, ])
}

#predict from a full model
helmet$pred <- predict(lm(lunch ~ helmet, helmet))

#get RMSE of full model and CV set
rmse(helmet$pred, helmet$lunch)

rmse(helmet$cv_pred, helmet$lunch)
```

# Issues to Consider 

## categorical variables

We can use the function `model.matrix` to see how r deals with modeling categorical variables.

**Interpreting model estimates for categorical predictors**
> The estimated intercept is the value of the response variable for the first category (i.e. the category corresponding to an indicator value of 0). The estimated slope is the average change in the response variable between two categories.


```{r}
library(MASS)

#Examine the dataset
glimpse(whiteside)

#create formula object
fmla <- as.formula("Gas ~ Temp + Insul")

#examine the structure of the model
head(model.matrix(fmla, whiteside, 5))
```


```{r}
#create a model using the formula
white_mod <- lm(fmla, whiteside)

#examine the model result
summary(white_mod)

#predict the BP change for each observation
whiteside$pred <- predict(white_mod, whiteside)

#plot the predicted BP change vs. actual
ggplot(whiteside, aes(x = pred, y = Gas)) +
  geom_point() +
  geom_abline(color = "blue")
```

##Interactions

**interactions**

Recall that linear regression assumes an additive relationship between variables. 

plant height ~ sunlight + bacteria

Change in height is the sum of the effects of bacteria and light. Therefore, 
Change in sunlight causes the same change in height, independent of bacteria. And also,
change in bactera causes the same change in height, independent of sunlight.

**An interaction occurs when the simultaneous influence of two variables on the outcome is not additive.**

In a formula, you can specify interacion with a colon (:)

`y ~ a:b`

You can specify main effects AND interaction with an asterisk (*)

`y ~ a*b`  
  
is the same as
  
`y ~ a + b + a:b`
  
And finally, you can express the product of two variables using: I(*)
  
`y ~ I(a*b)`
  
## Transformations

Transformations can take place on the response variable before modeling. A common example of this is log transforming monetary values, which are often lognormally distributed (tend to be skewed with a long tail). The procedure for doing this is as follows:

1) Log the outcome and fit a model  
2) Maker the predictions in log space  
3) Transform the predictions back to the original outcome space.

Log-transformed outcomes have miltiplicative error. In order to get around this, it is best to determine the relative error.

```{r}

data(Sacramento, package = 'caret')

glimpse(Sacramento)

n <- nrow(Sacramento)

sample <- sample.int(n = n, size = floor(0.7 * n), replace = F) 

sac_train <- Sacramento[sample, ]
sac_test <- Sacramento[-sample, ]

sac_mod <- lm(price ~ sqft, sac_train)

glance(sac_mod)

sac_test$pred <- predict(sac_mod, sac_test)

sac_test <- sac_test %>%
  group_by(type) %>%
  mutate(residual = pred - price,
         relerr = residual/price)

#compre the rmse to the relative rmse
sac_test %>%
group_by(type) %>% 
  summarize(rmse = sqrt(mean(residual^2)),
            rmse.rel = sqrt(mean(relerr^2)))

#plot actual vs. predicted for each housing group
ggplot(sac_test, aes(x = pred, y = price, color = type)) + 
  geom_point() + 
  geom_abline() + 
  facet_wrap(~ type, ncol = 1, scales = "free") + 
  ggtitle("Outcome vs prediction")
```

Using the Cars93 dataset, we'll examine how log transforming the response can lead to better model results.
```{r}
glimpse(cars)

car_n <- nrow(Cars93)

car_sample <- sample.int(n = car_n, size = floor(.7 * car_n), replace = F)

car_train <- Cars93[car_sample, ]
car_test <- Cars93[-car_sample, ]

#make model based on log value of response variable
log_mod <- lm(log(Price) ~ Horsepower + MPG.city, car_train)

#compare this to a regular model
car_mod <- lm(Price ~ Horsepower + MPG.city, car_train)

#compare the models
glance(log_mod)$r.squared
glance(car_mod)$r.squared

#make predictions on the test set
car_test$pred <- predict(log_mod, car_test)

#convert predicted values back to original units
car_test$pred_price <- exp(car_test$pred)

ggplot(car_test, aes(x = pred_price, y = Price)) +
  geom_point() +
  geom_abline(color = "red") 
```

# Dealing with Non-Linear Responses


### logistic regression
Predicting whether an even occurs (yes/no) fals into the realm of classification.   

However, predicting the probability that an event occurs is regression.

For the latter, we can use **logistic regression**, which assumes inputs are additive, and linear in log-odds. The log-odds is the ratio between
the probability of an event occurring to the probability of the event not occurring.

The `glm` function is used for logistic regression and functions similarly to the `lm` function with the addition of the `family` argument. For logistic regression, the family argument will be set to **binomial**.

To evaluate a logistic regression model, the deviance and pseudo $R^2$ are used. The deviance is similar to the variance.

**titanic or biopsy dataset for logit regression**

```{r}
glimpse(biopsy)

biopsy <- biopsy %>%
  dplyr::select(-ID)

biopsy.fmla <- as.formula("class ~ .")

logit_mod <- glm(formula = biopsy.fmla, family = binomial, data = biopsy)

summary(logit_mod)

glance(logit_mod)

(pseudoR2 <- 1 - (glance(logit_mod)$deviance/glance(logit_mod)$null.deviance))

```


### poisson and quasipoisson regression

# epil dataset for poisson

### GAM

Generalized Additive Models.

`mgcv` package for the `gam` function.

`s()` function to denote a non-linear relationship of a ocntinuous variable in your formula.

```{r}
#install.packages("mgcv")
library(lubridate)
library(mgcv)

#exmaine the data
glimpse(goog)
#convert date variable to date format
goog$Date <- as_date(goog$Date)

#convert date variable to continuous sequence 
#for comptability with splining in formula
goog$d <- seq(from = nrow(goog), to = 1)

#split data into train and test sets
n <- nrow(goog)

sample <- sample.int(n = n, size = floor(0.75 * n), replace = F) 

goog_train <- goog[sample, ]
goog_test <- goog[-sample, ]


#visualize the variables of interest in the training set
ggplot(goog, aes(x = Date)) +
  geom_point(data = goog_train, aes(y = Close, color = "blue")) +
  geom_point(data = goog_test, aes (y = Close, color = "green")) +
  scale_color_manual(name = "dataset", values = c('blue' = 'blue', 'green' = 'green'), labels = c("train", "test"))

#Fit GAM model on training set
model.gam <- gam(formula = Close ~ s(d), data = goog_train, family = gaussian)

#fit lm model on training set and compare
model.lm <- lm(formula = Close ~ d, data = goog_train)

summary(model.gam)
summary(model.lm)

plot(model.gam)

#get predictions for linear model
goog_test$pred.lm <- predict(model.lm, goog_test)

#get predicitons from gam model
#use as.numeric to convert matrix output to vector
goog_test$pred.gam <- as.numeric(predict(model.gam, goog_test))

#gather predictions into a long dataset
goog_long <- goog_test %>%
  gather(key = modeltype, value = prediction, ... = pred.lm, pred.gam)

#calculate the rmse
goog_long %>%
  mutate(residual = Close - prediction) %>%
  group_by(modeltype) %>%
  summarise(rmse = sqrt(mean(residual^2)))

goog_long %>%
  ggplot(aes(x = Date)) +
  geom_point(aes(y = Close)) +
  geom_point(aes(y = prediction, color = modeltype)) +
  geom_line(aes(y = prediction, color = modeltype, linetype = modeltype)) +
  scale_color_brewer(palette = "Dark2")
```

# Tree-based Methods

## Random Forests

Multiple diverse decision trees averaged together.

Reduces overfit
Increases model expresiveness
Finer grain predictions

`ranger` package for the function with the same name that will perform random forest modeling.

```{r}
install.packages("ranger")
library(ranger)

ranger(formula = , data = , num.trees = , respect.unordered.factors = , seed = )
```


## One-hot Encoding

**categorical variables** - one-hot-coding

The model will leave out one of the categories as this is the reference-level. The coefficient value tells you the change expected when all other variables are 0 relative to the reference level.
One-hot encoding is required to convert categorical values to numerical